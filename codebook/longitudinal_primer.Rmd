--- 
title: "The Hitchhiker’s Guide to Longitudinal Models"
subtitle: "Code Companion"
author: "[Ethan M. McCormick](https://mccormickneuro.github.io/)"
date: "Published: `r format(Sys.time(), '%d %B, %Y')`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
#link-citations: yes
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a code companion for The Hitchhiker’s Guide to Longitudinal Models.
link-citations: yes
url: https\://https://mccormickneuro.github.io/longitudinal-primer/
github-repo: "McCormickNeuro/longitudinal-primer"
---

# About {#about}
```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following document is a code companion to [The Hitchhiker’s Guide to Longitudinal Models: A Primer on Model Selection for Repeated-Measures Methods](https://psyarxiv.com/ga4qz), [https://osf.io/bn6yu/](https://osf.io/bn6yu/). 

Some general notes about this code companion:

- We believe in the importance of using real data in our examples of longitudinal models. However, some of the models we discuss can not yet be fit using publicly-available neuroimaging data (most often due to a limited number of observations). To bridge this gap, we have synthesized data from a number of sources, detailed in [Datasets](#datasets). Variable names and identification codes have been changed to protect the innocent and to provide examples that will be familiar to developmental cognitive neuroscience researchers. However, one limitation of synthesized data is that model fits are often significantly worsened compared to the real data. As such, for pedagogical purposes, we will fit (and sometimes interpret) results from models that we would usually reject in practice based on overall model fit.

- Given the nature of R, there are likely several ways to accomplish what we outline here. This code should not be taken as the *definitive* one way that data manipulation or model fitting can be accomplished, but rather as a standard pipeline that should hopefully be accessible to new users of R and longitudinal methods more generally.

- In general, we will include the code used to generate the results. However, in the interest of spending more time on the models and associated syntax, we may not include an in-depth explanation of every function or syntax option. The rmarkdown source files are available in a public [github repository](https://github.com/McCormickNeuro/longitudinal-primer) for those interested.

- While the contents of this primer and code companion will focus on implementation in the R environment, links will be provided to other programs where possible. **A fair warning: code in other languages will be provided in an arbitrary and capricious manner. This policy should be familiar to the reader, given their experience with the text of the manuscript (and if very unlucky, the first author).**

- In general, precision beyond the third decimal place is as real as [unicorns](https://psycnet.apa.org/record/1989-14214-001) and so we will round our numbers to that level when discussing them.

<!--chapter:end:index.Rmd-->

# Introduction {#intro}

The first thing we can do is run a function that will load the R packages needed for this code companion, and install them to our machine if they are not already.

```{r, packages, message = FALSE, warning = FALSE, error = FALSE}
packages <- c("utils", "tidyverse", "downloadthis",     # packages for data management
              "foreign", "MplusAutomation",             # packages for writing data         
              "sjPlot", "broom", "kableExtra",          # packages for generating tables
              "nlme", "lme4", "lmerTest", "stats",      # packages for MLMs
              "mgcv", "gamm4", "itsadug",               # packages for GAMMs
              "lavaan",                                 # packages for SEMs
              "ggplot2", "semPlot", "ggeffects",        # packages for visualization   
              "interactions")                           # packages for probing interactions

if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, 
                           rownames(installed.packages())), 
                   repos = "http://cran.us.r-project.org")
}
invisible(lapply(packages, 
                 library, 
                 character.only = TRUE))
```

We have automatically generated a downloadable bibliography of the R package versions used in this companion for later reproducibility.

```{r, write bibliography, message = FALSE, warning = FALSE, error = FALSE}
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "external/hitchhikers-guide-packages.bib")
```
```{r, download external 01, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
download_file(
  path = "external/hitchhikers-guide-packages.bib",
  output_name = "hitchhikers-guide-packages",
  button_label = "Download Bibliography file",
  button_type = "default",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

<!--chapter:end:01-introduction.Rmd-->

# Canonical Models {#canon}

What follows are canonical versions of growth models in each of the four different frameworks. These models represent basic implementations of a linear growth trajectory with random effects for both the intercept and slope, with the exception of the GAMM, where a non-linear spline model is implemented (otherwise it would just re-capitulate the MLM results). This will be the longest chapter of the codebook since we will cover syntax and model output more in-depth than in later chapters. Remain calm and clutch your towel as necessary.

First, we need to read in the datasets we will use in this chapter.

```{r, read data canonical, message = FALSE, warning = FALSE, error = FALSE}
executive.function <- utils::read.csv("data/executive-function.csv", header = TRUE) %>%
  select(id, dlpfc1:dlpfc4)

executive.function.long <- executive.function %>% 
  tidyr::pivot_longer(cols = starts_with("dlpfc"), 
                      names_to = c(".value", "wave"), 
                      names_pattern = "(.+)(.)") %>%
  dplyr::mutate(wave = as.numeric(wave) - 1)

feedback.learning <- read.csv("data/feedback-learning.csv") %>% 
  select(id, age, modularity)
```

While we will usually read in the wide and long versions of the data directly, here we will demonstrate how to reformat the more common wide data format (i.e., each row corresponds to a different individual and repeated measures are new variables) into the long data format (i.e., each row corresponds to a different repeated measure with multiple rows per person). We can use the `pivot_longer()` function from the [**tidyr**](https://tidyr.tidyverse.org/) package (alternatives include `melt()` from the [**reshape**](https://cran.r-project.org/web/packages/reshape2/index.html) package). We will collect all the columns that begin with the string `dlpfc` and pass the values into one column, while creating a new column `wave` from their number indices (e.g., `1` for `dlpfc1`). We can then use the `mutate()` function from the [**dplyr**](https://dplyr.tidyverse.org/) package to change the wave colum from a character to a numeric variable and subtract $1$ from every value so that the first wave is coded as $0$ (this will be important for interpretations later).

Details regarding these datasets can be found in [Datasets](#datasets). However, shortly, the `executive-function.csv` file contains data for $342$ adolescents measured up to $4$ times. At each wave, adolescents played an executive function task while in an fMRI scanner. For now, we will only use the DLPFC measures to build the canonical growth models. The first $5$ individuals are shown below. These data are in wide format.

```{r, display executive.function wide 02, message = FALSE, warning = FALSE, error = FALSE}
executive.function %>% filter(id <= 5) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Executive Function Data: Wide Format**",
                    align = "c",
                    row.names = FALSE) %>%
  kableExtra::row_spec(row = 0, align = "c")
```

We can then see what this looks like in long format.

```{r, display executive.function long 02, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long %>% filter(id <= 5) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Executive Function Data: Long Format**",
                    align = "c",
                    row.names = FALSE) %>%
  kableExtra::row_spec(row = 0, align = "c")
```

The `feedback-learning.csv` file contains data for $297$ adolescents and young adults measured up to $3$ times. At each wave, individuals played a feedback learning task while in an fMRI scanner. ROI timeseries were then used to construct a brain network graph and the modularity of that graph was calculated. Here we will focus on these modularity values. The first $5$ individuals are shown below.

```{r, display feedback.learning 02, message = FALSE, warning = FALSE, error = FALSE}
feedback.learning %>% filter(id <= 5) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Feedback Learning Data**",
                    align = "c",
                    row.names = FALSE) %>%
  kableExtra::row_spec(row = 0, align = "c")
```

Before we move into fitting the growth models in each of the four frameworks, we can begin by exporting the data to [**SAS**](https://www.sas.com/) and [**Mplus**](https://www.statmodel.com/) compatible file types. We will use the [**MplusAutomation**](https://michaelhallquist.github.io/MplusAutomation/articles/vignette.html) package to write our Rdata into a `.dat` file. We have wrapped the command in the `capture.output()` function so that we can retain a record of the column names (**Mplus** does not accept files with headers) to prevent referencing the wrong variable when we run our model. The output of the **MplusAutomation** command `prepareMplusData()` will be saved in a text file in the same external directory.

```{r, write to mplus 02, message = FALSE, warning = FALSE, error = FALSE}
filename <- "executive-function"
capture.output(
  MplusAutomation::prepareMplusData(executive.function, 
                                    paste0("external/",filename,".dat")),
  file=paste0("external/",filename,"_MplusAutomation.txt"))
```

Although we do not show the running and output of these models in-depth, all the **Mplus** files needed to recreate the MLM and SEM models are available to be downloaded below.

```{r, download mplus 02, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
download_file(
  path = c(paste0("external/",filename,".dat"),
           paste0("external/",filename,"_MplusAutomation.txt")),
  output_name = "02-canonical-mpluscode",
  button_label = "Download Mplus files",
  button_type = "default",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

For readers interested in how to automate running **Mplus** models through **R**, please consult the extrememly useful functions in [**MplusAutomation**](https://michaelhallquist.github.io/MplusAutomation/articles/vignette.html) that allow you to generate syntax, run saved input files, and re-import **Mplus** models into the R environment. Please note that a local **Mplus** license is still necessary to run these models.

We can also use the package [**foreign**](https://cran.r-project.org/web/packages/foreign/index.html) to write out the data to a **SAS** compatible text file. The `write.foreign()` function also outputs a syntax file that imports the data into the **SAS** environment. 

```{r, write to sas 02, message = FALSE, warning = FALSE, error = FALSE}
foreign::write.foreign(executive.function.long, 
                       datafile = paste0("external/",filename,".txt"), 
                       codefile = paste0("external/import_",filename,".sas"), 
                       package="SAS")
```

For convenience, all the **SAS** files needed to recreate the MLM analyses are available for download below.

```{r, download sas 02, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
download_file(
  path = c(paste0("external/",filename,".txt"),
           paste0("external/import_",filename,".sas")),
  output_name = "02-canonical-sascode",
  button_label = "Download SAS files",
  button_type = "default",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

## Multilevel Model

We will start with the multilevel growth model using our `executive.function` data. There are a number of package options to fit multilevel models, but we will focus here on two of the most popular: [**nlme**](https://cran.r-project.org/web/packages/nlme/index.html) and [**lme4**](https://cran.r-project.org/web/packages/lme4/index.html). We will also use the [**lmerTest**](https://cran.r-project.org/web/packages/lmerTest/index.html) package which outputs *p*-values for the tests, which are not included in the **lme4** package. We will fit a very simple linear model with a random intercept and slope for the DLPFC activation data using all of these packages. For these initial models, we will mostly use `wave` as our metric of time rather than `age` for simplicity. We will return to models which use different time variables in later sections.

### nlme

The **nlme** function `lme()` separates its syntax into a `fixed` and `random` argument using the standard `glm()` syntax common to **R**. In the `random` argument, we indicate that the random intercept (`1`) and random slope (`wave`) are nested within `id` using the pipe (`|`) character. We will indicate that rows with missing data will be omitted using `na.action = na.omit` and set the estimator to REstricted Maximum Likelihood (REML). For a discussion of REML versus Full Information Maximum Likelihood (FIML; set `method = "ML"`), see more information [here](https://www.tandfonline.com/doi/abs/10.1080/00273171.2017.1344538). Once the model is finished running, we can print the model output to the console using the `summary()` function. Here we use the argument `correlation=FALSE` to suppress the correlation of fixed effects output, which is unlikely to be of interest to applied researchers.

```{r canonical MLM 01, warning = FALSE, message = FALSE}
mlm.nlme <- nlme::lme(dlpfc ~ 1 + wave,
                      random = ~ 1 + wave | id,
                      na.action = na.omit,
                      method = "REML",
                      data = executive.function.long)

summary(mlm.nlme, correlation = FALSE)
```

The first section of the model output shows the loglikelihood (`logLik`) and associated absolute fit indices (`AIC` and `BIC`). The `Random effects` section shows parameter estimtaes of the standard deviations (`StdDev`) and correlations (`Corr`) of the random effects (the identities are denoted by labels to the left of the matrix), as well as the level-1 error standard deviation (`Residual`). We can see that the standard deviation of the random intercept ($StdDev = 0.812$) is substantially larger than that of the random slope ($StdDev = 0.223$). This is often the case and has more to do with the scaling of the effect than anything else. The correlation between the random effects ($Corr = -0.354$) suggests that individual with higher initial levels of DLPFC activation show lower slope values . We will need to interpret this in the context of the fixed effects (e.g., are the slopes less positive, or more negative?), and plotting the predicted effects is often useful for this interpretation.

The `Fixed effects` section contains the regression estimates of our fixed effects. Here we can see that the average value of DLPFC activation at the intercept is $0.546$. Because we coded the `wave` predictor to have values of $0$ at the first measurement occasion, this means that the intercept corresponds to initial DLPFC activation. Of course, the units here aren"t meaningful, but initial status is often (but not always) the desired interpretation of the intercept term. Here the slope is positive and significant ($\gamma_{wave} = .199, SE = 0.021, p < .001$).

The final section contains information about the number of observations and number of groups, which are good values to check to ensure that the nesting of the data in the model corresponds to expectations.

### lme4

The `lmer()` function has largely supplanted `lme()` in most applications of multilevel models. Here, the formula is contained within a single line but is largely the same format. Additionally, the `method = "REML"` argument has been changed to `REML = TRUE` (`REML = FALSE` gives us FIML).

```{r canonical MLM 02, warning = FALSE, message = FALSE}
mlm.lme4 <- lme4::lmer(dlpfc ~ 1 + wave + (1 + wave | id), 
                       na.action = na.omit,
                       REML = TRUE,
                       data = executive.function.long)

summary(mlm.lme4, correlation = FALSE)
```

The main differences in the output are that `lmer()` gives the variance of the random effects in addition to the standard deviations, and that there are no longer *p*-values associated with the fixed effects. However, all the values are identical to the `lme()` solution (which should make us sigh in relief).

If we wish to extract significance information on the random effect components, we can calculate confidence intervals using the `confint()` function from the [**stats**]() package.

```{r canonical MLM 02.1, warning = FALSE, message = FALSE}
confint(mlm.lme4)
```

For each parameter, we want to check if the confidence interval contains $0$. Unfortunately, the standard command labels things in non-specific terms (e.g., `.sig01`). If we want more informative labels, we can include the `oldNames = FALSE` argument.

```{r}
confint(mlm.lme4,
        oldNames = TRUE)
```

As we can see, these labels are much more informative to ensure we are interpreting the correct parameter. Note: `sigma` will always be the level $1$ residual variance and will not have an informative label. The corresponding command for computing confidence intervals in **nlme** is `intervals()`.


### lmerTest

If we wish to retain *p*-values in our solution, we can load the package **lmerTest** and use its version of `lmer()` to fit the model. The syntax is identical to what we used above, and the parameter estimates are as well.

```{r canonical MLM 03, warning = FALSE, message = FALSE}
mlm.lmerTest <- lmerTest::lmer(dlpfc ~ 1 + wave + (1 + wave | id),
                               na.action = na.omit,
                               REML = TRUE,
                               data = executive.function.long)

summary(mlm.lmerTest, correlation = FALSE)
```

### MLM Outputs

The `summary()` function output, while extensive, is not formatted for publication. However, we can use a great function `tab_model()` from the [**sjPlot**](https://strengejacke.github.io/sjPlot/) package to generate publication-quality tables from the MLM output. Here we will merge the results from the **nlme** and **lmerTest** packages.

```{r canonical MLM 04, warning = FALSE, message = FALSE}
sjPlot::tab_model(mlm.nlme, mlm.lmerTest,
                  show.se = TRUE,
                  show.df = FALSE,
                  show.ci = FALSE,
                  digits = 3,
                  pred.labels = c("Intercept", "Wave"),
                  dv.labels = c("nlme", "lme4"),
                  string.se = "SE",
                  string.p = "P-Value")
```

If we wish, we can output the table into a file that we can use to incorporate it into a document using the argument `file = "/path/to/output/sjPlot_table.html"`.

### MLM Plotting Model-Implied Trajectories

If we want to plot the model-implied trajectories for each individual, we can use the [**ggplot2**](https://ggplot2.tidyverse.org/) package. To plot data, we need to have it in long format. Fortunately this is the format used in the MLM model so we don"t need to do anything additional. We will plot predicted values generated from the `predict()` function. While we could append these values to our `executive.function.long` dataframe in a separate step, we will instead generate the values locally within the `ggplot()` function. This will save us a step and we won"t have to deal with merging the predicted values into our dataframe or having to remove those values later. Because MLMs drop `NA` values, our predicted values will not match up to the original dataframe unless we also drop thos `NA` values, so we will use the `drop_na()` function from **tidyr**.

```{r canonical MLM 05, warning = FALSE, message = FALSE}
ggplot2::ggplot(tidyr::drop_na(executive.function.long), 
                aes(x = wave + 1, 
                    y = predict(mlm.lmerTest), 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Canonical MLM Trajectories",
       x = "Wave",
       y = "Predicted DLFPC Activation") +
  theme(legend.position = "none") 
```

We can see that there is quite a bit of individual differences both in the initial level, but also in slopes across time, with some individuals showing increases but other showing no change or even decreases across waves. A quick tip is to always use the `theme(legend.position = "none")` function in this kind of plot unless you want the plot to try to show you each individual with the color of their trajectory line.

## Generalized Additive Mixed Model

We can move on to fitting a GAMM model. Unlike the other frameworks, we will use the `feedbacklearn` data with continuous age across 2 decades in this model. We could, in theory, fit the `executive.function` data, but that would largely be a waste of the GAMM framework"s potential for longitudinal analysis. Here we will utilize the [**gamm4**](https://cran.r-project.org/web/packages/gamm4/index.html) package for fitting GAMMs with the `gamm4()` function, although other options (e.g., `gamm()` or `gam()` from the [**mgcv**](https://cran.r-project.org/web/packages/mgcv/index.html) package). Fortunately, the `gamm4()` uses syntax that we are largely familiar with from fitting the MLMs in the prior section. Here we will begin by fitting a simple smooth function for values of network modularity across age. Note here that we standardize the modularity values since the natural scale results in very small variance components, and the units of modularity are not naturally meaningful anyways. To invoke the smoothing function, we wrap the `age` predictor with the `s()` function in the formula. Note that the random effects argument resembles the **nlme** syntax. For now we will allow many of the defaults to kick in, but will investigate them in turn.

```{r canonical GAMM 01, warning = FALSE, message = FALSE}
gamm <- gamm4::gamm4(scale(modularity) ~ 1 + s(age),
                     random = ~ (1 | id),
                     data = feedback.learning)
```

Unlike the MLM models, the GAMM results in two sets of model results from linear and nonlinear parts of the model. To access the linear model we need to specify the `mer` part of the model output. 

```{r}
summary(gamm$mer, correlation = FALSE)
```

We will ignore the `Fixed effects:` portion of the output for now because our main effect of interest is the nonlinear smooth of age and we did not include additional linear effects (we will see this more later). Focusing on the `Random effects:`, we can see that there is a little more person-to-person variability (`Groups = id`, `Name = (Intercept)`) than within-person variability (`Groups = Residual`), but they are roughly equivalent. Note that we have an "extra" random effect (`Groups = Xr`, `Name = s(age)`). Although this might lead us to believe that we have included a random within-person slope, but it is a formulation of the smooth itself as a random effect and is not terribly informative here. We *could* include a true random effect of age by expanding the formula in the `random` argument (`random = ~ (1 + age | id)`), but that model is not identified in this data and so we won't.

We can then take a look at the nonlinear portion of the output by summarizing the `gam` portion of the model.

```{r}
summary(gamm$gam)
```

As before, we will mostly ignore the `Parametric coefficients:` portion (although see that the `(Intercept)` estimate is identical; this should be of concern if this wasn't the case). The `Approximate significance of smooth terms:` portion shows that we have a significant effect of age with a estimated degrees of freedom (EDF) of $4.65$. The EDF give us a shrunken estimate of the smooth complexity (higher values indicate greater complexity). However, this isn't incredibly helpful to understand the nature of the developmental effect. Instead, let's turn to the output options and see what we can find there.

### GAMM Outputs

Just like with MLMs, we can generate pretty tables instead of just manually typing out the console output into a word processor. We can use the `tab_model()` for the `mer` part of the output, but we need to call the `gamtabs()` function from the [**itsadug**](https://cran.r-project.org/web/packages/itsadug/index.html) package for the `gam` section.

```{r canonical GAMM 02, warning = FALSE, message = FALSE, results="asis"}
itsadug::gamtabs(gamm$gam, type = "html",
                 pnames = c("Intercept"), snames = c("s(Age)"),
                 caption = "Modularity as a Function of Age")
```

To visualize the results, we can call the `plot.gam()` function from the **mgcv** package (which aliases as `plot()` when the **mgcv** package is loaded). We can include the standard errors and a rug plot to show the individual age observations in our data. This can help us identify regions of the estimated trajectory that are more or less supported by the data in addition to the standard error width.

```{r}
mgcv::plot.gam(gamm$gam, se = TRUE, rug = TRUE, shade = TRUE,
               xlab = "Age", ylab = "Fitted Modularity Values")
```

The resulting plot tells a rather interesting story about the developmental trajectory of modularity, with initial increases across early adolescence, a plateauing in mid-adolescence, and then slow declines into young adulthood. However, we can see there is a lot more uncertainty at the later ages (indicated by the sparsity in the hashes in the rug plot).

## Latent Curve Model

We will now turn to the longitudinal modeling within the SEM framework, beginning with the latent curve model (or latent growth model; quantitative people aren"t the best with consistent terminology sometimes). Unlike with the mixed-effects models, we will focus on a single package, [**lavaan**](https://lavaan.ugent.be/), which has become the workhorse of structural equation modeling natively in **R** (for running SEMs in **Mplus** using **R** commands, see the **MplusAutomation** package). Like with the MLM, we will fit a simple linear growth model using `wave` as our metric of time. However, unlike with the MLM, time will not appear as a specific variable in the model; rather we code time measurements directly into the factor loadings. 

### LCM Syntax and Model Fitting

First we will define a model syntax object that specifies the model. While we will cover the basics here, consult the [**lavaan website**](https://lavaan.ugent.be/tutorial/syntax1.html) for a more complete syntax tutorial.

```{r canonical LCM 01}
linear.lcm <- "
              # Define the Latent Variables
              int =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
              slp =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4
              
              # Define Factor Means
              int ~ 1
              slp ~ 1
              
              # Define Factor (Co)Variances
              int ~~ int
              int ~~ slp
              slp ~~ slp
              
              # Define Indicator Residual Variances
              dlpfc1 ~~ dlpfc1
              dlpfc2 ~~ dlpfc2
              dlpfc3 ~~ dlpfc3
              dlpfc4 ~~ dlpfc4
"
```

In the first section, we use the `=~` operator to define the latent variables (`int` is the intercept and `slp` is the linear slope factor). The intercept factor loadings are set by pre-multiplying the individual indicators (`dlpfc1-dlpfc4`) by values of $1$. We define the slope by pre-multiplying the indicators by linearly increasing values. Here we set the first factor loading to $0$ and each subsequent loading by increasing integers. This has the effect of estimating intercept values that reflect levels of DLPFC activation at the initial observation (where time is coded $0$) and a slope effect that is expressed in per-wave units (here this relates to per-year changes). Because of defaults built into the **lavaan** function `growth()`, we could estimate the full linear LCM with just these first two lines of code (the **Mplus** syntax is similarly simple). However, for completeness, we will write out the remainder of the model parameters for this initial model.

The next two sections define the parameters of the latent variables. Here we estimate intercepts (i.e., the fixed or average intercept and slope) using the regression operator (`~`) with $1$ on the right hand side and (co)variances using the `~~` operator. Note that variances `int ~~ int` are just the covariance of a variable with itself (take a shot if that makes your head hurt a little bit). Finally, we can define the residual variances of the indicators using the same `~~` operator.

We can then use the `growth()` function mentioned previously to fit the syntax we wrote to the `executive.function` data. Here we will estimate the model with Maximum Likelihood (`estimator = "ML"`) and we will allow for missing data using the `missing = "FIML"` argument. Standard alternatives might include `estimator = "MLR"` for Robust Maximum Likelihood if we have non-normal continuous data, or `estimator = WLSMV` if we have discrete data (see [the lavaan webpage](https://lavaan.ugent.be/tutorial/est.html) for a full list of available options). In general, we will always allow for missing data, but we could change to `missing = "listwise"` if we wanted to do only complete-case analysis. This option is actually the **lavaan** default, so users should be cautious that this is intended and review the number of observations used in the model to confirm intended behavior.


```{r canonical LCM 02}
lcm <- lavaan::growth(linear.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")
```

### LCM Outputs

We can then summarize the model output using `summary()` with some optional arguments. While we will mostly output all of the available information, including `fit.measures`, raw (`estimates`) and standardized (`standardize`) parameter estimates, and `rsquare`, there might be cases where we are only interested in some subsection of the output. For instance, if we are building a sequence of models, we might only consult fit measures without viewing the parameters and associated inference tests so that our model selection isn"t driven by "peeking" at effects of interest.

```{r canonical LCM 03}
summary(lcm, fit.measures = TRUE, estimates = TRUE, 
        standardize = TRUE, rsquare = TRUE)
```

One nice thing about fitting our model to the raw data instead of standardizing beforehand is that with the `standardize = TRUE` argument, we get 3 sets of parameter estimates: the raw scale estimates (under `Estimates`), the estimates when the latent variables are standardized but the indicators remain in the raw scale (under `Std.lv`), and the fully standardized solution (under `Std.all`). In the same spirit of maximizing the information from the model, we can actually re-fit the model using `estimator = "MLR"`. Here we will only output the `fit.measures` to see how the robust estimator changes our model fit information.

```{r canonical LCM 04}
lcm <- lavaan::growth(linear.lcm, 
                      data = executive.function,
                      estimator = "MLR",
                      missing = "FIML")

summary(lcm, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

We can see that we get a new column of robust fit statistics to accompany the standard measures which correct for the non-normality in our data. In this case, our data are not too non-normal and so there is little difference. However, this will not always be the case. To vastly over-simplify, we are looking to see if our `Model Test User Model:` test statistic is non-significant. However, be aware that this test is over-powered and will often be significant in large samples, even in a well fitting model. We also tend to look for CFI/TLI $> 0.95$, RMSEA $< 0.05$, and SRMR $< 0.08$ to indicate an excellent model fit. While these cutoff values are somewhat arbitrary, they can serve as a rough huristic, and the linear LCM more than satisfies each of these criteria. For more discussion of fit indices, consult the resources outlined in the main text. We can now jump back and orient to the parameter estimate output.

```{r canonical LCM 05}
summary(lcm, fit.measures = FALSE, estimates=  TRUE, 
        standardize = TRUE, rsquare = TRUE)
```

The first section of parameters `Latent Variables:` is of little interest to us for now since we pre-determined these factor loadings as a part of our model (but maybe check that they are the values you expect) and there are therefore no inferential tests on these parameters. The `Covariances:` section shows us the covariance (and correlation if we asked for standardized results) between the intercept and slope. Here we can see that the correlation is strong and negative ($r = -0.499$) suggesting that those with the lowest initial levels of DLPFC activation tend to show the strongest increases in activation over time. 

The `Intercepts:` section shows us the means of the latent factors and would show the conditional (denoted by a `.` before a variable name) intercepts of the indicators, but we do not estimate these values in a growth model (rather the means are reproduced by the factor means through the loadings, $\mathbf{\alpha\Lambda}$). Here we can see that the average activation at the initial timepoint is $0.543$ and the average rate of change is $0.121$ units per wave, both of which are significant.

Next we have the factor variances and the indicator residual (again denoted with `.`) variances in the `Variances:` section. The variances of the intercept and slope are significant suggesting there are meaningful individual differences in initial level *and* rate of change over time. The residual variances comprise the variance of the indicator not accounted for by the latent variables in the model.

Finally, we have the `R-Square:` section where the proportion of variance explained in each of the endogenous variables (here just the indicators but this could contain other endogenous observed or latent variables in other models). Conveniently, this is simply $1$ minus the standardized residual variance for each item. 

We can output these parameters in a slightly more compact format using the `tidy()` function from the [**broom**](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package and pass it to the `kable()` function from the [**kableExtra**](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html) package to output a table in the `"html"` format. If we were writing a manuscript, we might alternatively wish to output the rmarkdown in PDF form and use the `format = "latex"` argument.

```{r canonical LCM 06}
broom::tidy(lcm) %>% 
  select(-op, -std.nox) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Linear Latent Curve Model**",
                    align = "c",
                    col.names=c("Parameter", "Estimate", "SE", "Statistic",
                                "*p*-value", "Std.LV", "Std.All"),
                    row.names = FALSE) %>% 
  kableExtra::row_spec(row = 0, align = "c")
```

### LCM Path Diagrams

We might also wish to generate a diagram visualization of the LCM model, either for model checking or for presenting results. We can use the `semPaths()` function from the [**semPlot**](http://sachaepskamp.com/semPlot/examples) package to do just that. Here we will plot the model results including the intercepts and with black paths. The `what` argument we will set to `"est"` to scale the size of the paths to the parameter estimate.

```{r canonical LCM 07}
semPlot::semPaths(lcm,
                  what = "est",
                  intercepts = TRUE, 
                  edge.color = "black")
```

However, we can see that this path scaling is a little unfortunate because the integer factor loadings sort of swamp out the parameters we are primarily interested in. Instead we can change this argument to `what = "paths"` to have equally-sized paths, and then label those paths with the parameters using `whatLabels = "est"` (or `whatLabels = "std"` for standardized estimates).

```{r canonical LCM 08}
semPlot::semPaths(lcm,
                  what = "paths",
                  whatLabels = "est",
                  intercepts = TRUE, 
                  edge.color = "black")
```

Much better... By standard convention, latent variables are represented as circles, observed variables as squares, regression paths as straight single-headed arrows, and (co)variances as curved double-headed arrows. Parameters that are set to particular values rather than estimated (e.g., factor loadings here and indicator intercepts) are displayed with dashed lines.

### LCM Plotting Model-Implied Trajectories

Finally, like with the MLM, we might want to plot model-implied individual trajectories of DLPFC activation. We have to do a little data management song and dance to get the predicted values and then convert to long format (which we do within the `ggplot()` function directly instead of creating a new dataframe to store in memory), but the results are the same as before.

```{r  canonical LCM 09}
ggplot2::ggplot(data.frame(id=lcm@Data@case.idx[[1]], 
                           lavPredict(lcm,type="ov")) %>% 
                  pivot_longer(cols = starts_with("dlpfc"), 
                               names_to = c(".value", "wave"), 
                               names_pattern = "(.+)(.)") %>%
                  dplyr::mutate(wave = as.numeric(wave)), 
                aes(x = wave, 
                    y = dlpfc, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Canonical LCM Trajectories",
       x = "Wave",
       y = "Predicted DLFPC Activation") +
  theme(legend.position = "none") 
```


## Latent Change Score Model

Finally, we can turn the LCSM parameterization of the linear growth model. Like the LCM, the time predictor will not appear in our model syntax, but even more strangely (at first), neither will *any* values of time. Rather we will sum across latent change factors to estimate the slope across time.

### Linear LCSM

The main complication of the LCSM syntax relative to what we saw with the LCM is that we need to generate phantom variables from the indicators and then relate them with fixed path coefficients to the latent change factors. 

```{r canonical LCSM 01}
linear.lcsm <- "
               # Define Phantom Variables (p = phantom)
               pdlpfc1 =~ 1*dlpfc1; dlpfc1 ~ 0; dlpfc1 ~~ dlpfc1; pdlpfc1 ~~ 0*pdlpfc1
               pdlpfc2 =~ 1*dlpfc2; dlpfc2 ~ 0; dlpfc2 ~~ dlpfc2; pdlpfc2 ~~ 0*pdlpfc2
               pdlpfc3 =~ 1*dlpfc3; dlpfc3 ~ 0; dlpfc3 ~~ dlpfc3; pdlpfc3 ~~ 0*pdlpfc3
               pdlpfc4 =~ 1*dlpfc4; dlpfc4 ~ 0; dlpfc4 ~~ dlpfc4; pdlpfc4 ~~ 0*pdlpfc4
        
               # Regressions Between Adjacent Observations
               pdlpfc2 ~ 1*pdlpfc1
               pdlpfc3 ~ 1*pdlpfc2
               pdlpfc4 ~ 1*pdlpfc3
        
               # Define Change Latent Variables (delta)
               delta21 =~ 1*pdlpfc2; delta21 ~~ 0*delta21
               delta32 =~ 1*pdlpfc3; delta32 ~~ 0*delta32
               delta43 =~ 1*pdlpfc4; delta43 ~~ 0*delta43
        
               # Define Intercept and Slope
               int =~ 1*pdlpfc1
               slp =~ 1*delta21 + 1*delta32 + 1*delta43
        
               int ~ 1
               slp ~ 1
               
               int ~~ slp
               slp ~~ slp
"
```

In the first section, we define the phantoms (e.g., `pdlpfc1`) with four commands (separated by `;` for compactness): 1) define the phantom by the indicator with a factor loading of $1$ (`pdlpfc1 =~ 1*dlpfc1`), 2) set the intercept of the indicator to $0$ (`dlpfc1 ~ 0`), 3) estimate the residual variance of the indicator (`dlpfc1 ~~ dlpfc1`), and set the variance of the phatom to $0$ (`pdlpfc1 ~~ 0*pdlpfc1`). This, in effect, pushes the characteristics we need from the indicator variable to the phantom, which facilitates the model. 

In the nexttwo sections, we set regressions of $\gamma = 1$ between adjacent phantoms variables, define the latent change factor by the phantoms after the initial time point (e.g., `delta21 =~ 1*pdlpfc2`), and set the variance of the latent change factor to zero. This may not be apparent at first blush, but the regressions of $1$ between adjacent timepoints essentially push the differences in the outcome between observations up into the latent change factors (i.e., what is left over after residualizing out the prior observation).

Finally, we have the final definition of the intercept and slope factors. However, unlike the LCM, the intercept factor is only estimated from the initial phantom variable. The slope factor is defined from the latent change factors, but instead of linear factor loadings, all of these loadings are $1$ (this sums across the latent changes to give the overall linear trajectory).

Like before, we will take this syntax object and use it to fit the model we specify. Unlike the LCM, we will use the `sem()` function instead because the `growth()` defaults won"t serve our purposes with the LCSM. Otherwise, the arguments we will invoke will be the same. While we display it for completeness, we will not spend much time on this output as it exactly recreates the estimates for the LCM we already walked through (we promise; or scroll back up and check). We generate a path diagram for this model as well, but do not display parameter estimates to reduce clutter.

```{r canonical LCSM 02}
lcsm.linear <- sem(linear.lcsm, 
                   data = executive.function, 
                   estimator = "ML",
                   missing = "FIML")

summary(lcsm.linear, fit.measures = TRUE, estimates = TRUE, standardize = TRUE, rsquare = TRUE)

broom::tidy(lcsm.linear) %>% 
  arrange(op) %>%
  select(-op, -std.nox) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Linear Latent Change Score Model**",
                    align = "c",
                    col.names=c("Parameter", "Estimate", "SE", "Statistic",
                                "*p*-value", "Std.LV", "Std.All"),
                    row.names = FALSE) %>% 
  kableExtra::row_spec(row = 0, align = "c")

semPlot::semPaths(lcsm.linear,
                  layout = "tree2",
                  intercepts = FALSE, 
                  edge.color = "black")
```

### Linear LCSM with Proportional Change

To a first approximation, the model above is an overly complicated method for estimating the same model we could do with two lines in the LCM. However, this parameterization allows us to include effects that are *not* possible to include in the LCM. To model proportional change, we regression the latent change factor on the prior timepoint phantom variable (`delta21 ~ pdlpfc1`). This effect tests how change between timepoints depends on prior level on the outcome of interest, and can be used to model interesting non-linearities (especially exponential trends). While not strictly necessary, it is common practice to create an equality constraint across all proportional effects. Here we accomplish this constraint by pre-multiplying all of these regressions with the same label (`beta`). The rest of the syntax remains the same as above.

```{r canonical LCSM 03}
proportional.lcsm <- "
               # Define Phantom Variables (p = phantom)
               pdlpfc1 =~ 1*dlpfc1; dlpfc1 ~ 0; dlpfc1 ~~ dlpfc1; pdlpfc1 ~~ 0*pdlpfc1
               pdlpfc2 =~ 1*dlpfc2; dlpfc2 ~ 0; dlpfc2 ~~ dlpfc2; pdlpfc2 ~~ 0*pdlpfc2
               pdlpfc3 =~ 1*dlpfc3; dlpfc3 ~ 0; dlpfc3 ~~ dlpfc3; pdlpfc3 ~~ 0*pdlpfc3
               pdlpfc4 =~ 1*dlpfc4; dlpfc4 ~ 0; dlpfc4 ~~ dlpfc4; pdlpfc4 ~~ 0*pdlpfc4
        
               # Regressions Between Adjacent Observations
               pdlpfc2 ~ 1*pdlpfc1
               pdlpfc3 ~ 1*pdlpfc2
               pdlpfc4 ~ 1*pdlpfc3
        
               # Define Change Latent Variables (delta)
               delta21 =~ 1*pdlpfc2; delta21 ~~ 0*delta21
               delta32 =~ 1*pdlpfc3; delta32 ~~ 0*delta32
               delta43 =~ 1*pdlpfc4; delta43 ~~ 0*delta43
               
               # Define Proportional Change Regressions (beta = equality constraint)
               delta21 ~ beta*pdlpfc1
               delta32 ~ beta*pdlpfc2
               delta43 ~ beta*pdlpfc3
        
               # Define Intercept and Slope
               int =~ 1*pdlpfc1
               slp =~ 1*delta21 + 1*delta32 + 1*delta43
        
               int ~ 1
               slp ~ 1
               
               int ~~ slp
               slp ~~ slp
"

lcsm.proportional <- sem(proportional.lcsm, 
                         data = executive.function, 
                         estimator = "ML",
                         missing = "FIML")
```

We will just focus here on the parameter estimate for the proportional effect (`beta`), which is negative, but non-significant ($\gamma = -0.090, SE = 0.276, p = 0.745$). If we were to interpret this parameter, we would say that those with higher levels at prior timepoints tend to show less positive changes between adjacent timepoints (we would need to interpret these in light of the fixed effects to be certain if this is smaller increases or greater decreases). Interestingly, note that this is similar in kind to the factor covariance we interpreted in the LCM. If we examine that same parameter here, we can see that the correlation has been attenuated ($r = -0.295$) and is now non-significant. Indeed all the parameter estimates have now changed because we have introduced this new proportional  effect.

```{r canonical LCSM 04}
summary(lcsm.proportional, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

Like before, we can generate tables, a path diagram, and plot model-implied individual trajectories. Note that the proportional model does not generate noticeably non-linear implied trajectories due to the very weak proportional effect.

```{r canonical LCSM 05}
broom::tidy(lcsm.proportional) %>% 
  arrange(op) %>%
  select(-op, -std.nox) %>%
  kableExtra::kable(label = NA,
                    format = "html",
                    digits = 3,
                    booktabs = TRUE,
                    escape = FALSE,
                    caption = "**Linear Latent Change Score Model with Proportional Change**",
                    align = "c",
                    col.names=c("Parameter", "Label", "Estimate", "SE", "Statistic",
                                "*p*-value", "Std.LV", "Std.All"),
                    row.names = FALSE) %>% 
  kableExtra::row_spec(row = 0, align = "c")

semPlot::semPaths(lcsm.proportional,
                  layout = "tree2",
                  intercepts = FALSE, 
                  edge.color = "black")

ggplot2::ggplot(data.frame(id=lcsm.proportional@Data@case.idx[[1]], 
                           lavPredict(lcsm.proportional,type="ov")) %>% 
                  pivot_longer(cols = starts_with("dlpfc"), 
                               names_to = c(".value", "wave"), 
                               names_pattern = "(.+)(.)") %>%
                  dplyr::mutate(wave = as.numeric(wave)), 
                aes(x = wave, 
                    y = dlpfc, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Canonical Proportional LCSM Trajectories",
       x = "Wave",
       y = "Predicted DLFPC Activation") +
  theme(legend.position = "none") 
```

```{r, cleanup canon, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```

<!--chapter:end:02-canonical.Rmd-->

# Time Structure {#time}

Now that we have covered the basic forms of each of the four modeling frameworks, we can start thinking more deeply about how to include time in our longitudinal models. We will begin by visualizing different kinds of assessment schedules and how our models might change depending on the structure of our observations. We will read in subsets of datasets that appear elsewhere in this primer codebook that follow each of three exemplar assessment types. The `single.cohort` and `accelerated` datasets were drawn from the `executive.function` and `feedback.learning` datasets we saw in the canonical models chapter. The `multiple.cohort` dataset has been drawn from the `adversity` dataset (for details, see the [Datasets](#datasets) chapter), which measures white matter development across 8 waves. These data contain fractional anisotropy (FA) measures derived from the forceps minor (`fmin`), a white matter tract that spans the hemispheres of the medial prefrontal cortex. We will see this dataset again when we consider predictors in the [Covariates](#covariates) chapter.

```{r read data time, message = FALSE, warning = FALSE, error = FALSE}
single.cohort <- read.csv("data/single-cohort.csv", header = TRUE)

multiple.cohort <- read.csv("data/multiple-cohort.csv", header = TRUE)

accelerated <- read.csv("data/accelerated.csv", header = TRUE)
```

## Assessment Schedules

### Single Cohort Data

As we cover in the main text, single cohort studies are by far the most common in longitudinal modeling. Below we can plot the assessment schedule for the canonical models we worked with in the last chapter. To declutter the plot, we have selected 50 individuals from the larger dataset and ordered them by their age at the first assessment.

```{r single cohort 03, message = FALSE, warning = FALSE, error = FALSE}
set.seed(12345)
ggplot(single.cohort %>%
         pivot_longer(cols=starts_with("age"),
                      names_to = c(".value", "wave"), 
                      names_pattern = "(.+)(.)"),
       aes(x = age, y = id)) +
  geom_line(aes(group = id), size = .5) +
  geom_point(aes(color = as.factor(wave))) + 
  labs(x = "Age", y = "Individual", color = "Wave") + 
  theme(legend.position = "top")
```

We can see that there is some noise in the assessment schedule, as individuals are not observed at *exactly* intervals, however, we can see clear separation in ages between each wave. Because we cover this data extensively in the [Canonical Models](#canon) chapter, we will not show model fits here. However, we should note that because of the structure of this data, age and wave of assessment are highly collinear (and are more so the better job we do at observing individuals at regular intervals). This will impact our considerations about time later.

### Multiple Cohort Data 

A slightly more complex assessment schedule design is the multiple cohort design. In these designs, we have multiple, discrete ages at the first assessment which vary across cohorts. Here we can plot our `multiple.cohort` data, which is organized into three cohorts, beginning at ages $4$, $5$, and $6$. While at later ages the cohorts mix somewhat in terms of ages sampled, no individual is observed at consecutive ages. This highlights a key advantage of the multiple cohort design; they can often provide coverage of a wider developmental window without requiring additional subjects or waves of assessment using principles of planned missingness.

```{r multiple cohort 01, message = FALSE, warning = FALSE, error = FALSE}
ggplot(multiple.cohort %>% 
         pivot_longer(cols=starts_with("fmin"),
                      names_to = c(".value", "age"),
                      names_pattern = "(....)(.+)") %>%
         drop_na(fmin) %>% 
         mutate(age = as.numeric(age)), 
       aes(x = age, y = id)) +
  geom_line(aes(group = id), size = .5) +
  geom_point(aes(color = as.factor(age), shape = factor(cohort)), size=2) +
  labs(x = "Age", y = "Individual", color = "Wave", shape = "Cohort") +
  theme(legend.position = "top") + guides(color = "none")
```

Here we will demonstrate how one might model this type of data using the linear latent curve model for simplicity; however, we could implement this model in any of the 4 modeling frameworks we discussed. To preface the model results, the values of the `fmin` outcome were normalized to the first age assessed (`fmin4`) because of the relatively small natural scale of FA values. We have also read in the full `adversity` data file due to convergence issues with the subsample in `multiple.cohort`.

```{r multiple cohort 02, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
adversity <- read.csv("data/adversity.csv", header = TRUE)
```

```{r multiple cohort 03, message = FALSE, warning = FALSE, error = FALSE}
mc.wm.model <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                       1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
                slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 
                       4*fmin8 + 5*fmin9 + 6*fmin10 + 7*fmin11
"

mc.wm <- growth(mc.wm.model,
                data = adversity,
                estimator = "ML",
                missing = "FIML")
```

The real leverage that FIML gives us is apparent using this model. We can measure across $8$ ages despite no individual having more than $4$ observations. We can see the abbreviated results below.

```{r multiple cohort 04, message = FALSE, warning = FALSE, error = FALSE}
summary(mc.wm, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

With this model, we will be warned that coverage for certain pairwise combinations is $< 10\%$, which just reflects the fact that almost no individuals gave data at adjacent ages. We could inspect the model coverage using the `lavInspect()` function using the argument `what = "coverage"`. Note the values at or near $0$ on the 1-off diagonal (e.g., `fmin5` and `fmin4`).

```{r multiple cohort 05, message = FALSE, warning = FALSE, error = FALSE}
lavInspect(mc.wm, what = "coverage")
```

Finally, we can generate a path diagram, highlighting that the intercept and slope are estimated from all ages even though no individual is measured across all of those ages.

```{r multiple cohort 06, message = FALSE, warning = FALSE, error = FALSE}
semPaths(mc.wm, 
         intercepts = TRUE,
         edge.color = "black")
```

### Accelerated Design

The most complex assessment schedule is the accelerated longitudinal design, where no two individual are assessed at the same maturational state (usually age). While it isn't strictly necessary that we have some smooth continuum of starting ages, accelerated longitudinal studies most often follow this approach. We can see an example below.

```{r accelerated 01, message = FALSE, warning = FALSE, error = FALSE}
set.seed(12345)

ggplot(accelerated %>% group_by(id) %>% filter(length(unique(wave)) == 3) %>% 
         ungroup() %>% filter(id %in% sample(id, 100)), 
       aes(x = age, y = id)) +
  geom_line(aes(group = id), size = .5) +
  geom_point(aes(color = as.factor(wave))) + 
  labs(x = "Age", y = "Individual", color = "Wave", shape = "Cohort") + 
  theme(legend.position = "top")
```

Unlike the single or multiple-cohort data, where we have discrete timepoints of observations, here we have as many as three unique timepoints per person since individuals are unlikely to be the *exact* same age. While it is possible to fit this type of model using definition variables in an SEM (see the **Mplus** code for the TSCORE option), it is far more common to fit these assessment schedules with mixed effects models, where we can include precise age into the model as a continuous predictor. We can use the multilevel model to fit a quadratic growth curve to this data. Note that the linear and quadratic effects are fixed effects that are pooled across individuals, and that only the random intercept is included. We could include additional random effects if the data support them, although we would need sufficient numbers of observations within-person to do so.

```{r accelerated 02, message = FALSE, warning = FALSE, error = FALSE}
accel <- lmer(scale(modularity) ~ 1 + age + I(age^2) + (1 | id), 
              na.action = na.omit,
              REML = TRUE,
              data = accelerated)
```

We can see the results of this model below.

```{r accelerated 03, message = FALSE, warning = FALSE, error = FALSE}
summary(accel)
```

If we want to get the overall fixed effects trend, rather than individual trajectories (as we did in the [Canonical](#canon) models), we can use the `ggpredict()` function from the [**ggeffects**]() package to generate predicted values of the outcome at specific values of the predictor. This package is especially useful for generating **ggplot**-compatible dataframes when plotting interactions, but we can still use it for main effects (although a polynomial is a special form of an interaction). We can specify the levels of the predictor within the brackets (`[]`) or set them to `all` as we will here to get the full range of ages in our plot. We can then pass this dataframe to **ggplot** and plot the effect of age with confidence bands shaded.

```{r accelerated 04, message = FALSE, warning = FALSE, error = FALSE}
accel.effects = ggeffects::ggpredict(accel, terms='age [all]')

ggplot(data=accel.effects, aes(x=x,y=predicted)) + 
  geom_line() + 
  geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.2) + 
  labs(x='Age', y = 'Predicted Learning Rate')
```

We can see quite a pronounced inverted-U quadratic effect here, with a peak around twenty. While we have this plot here, we might wish to know if the simple linear slope is significant at various points along the curve (i.e., is modularity significantly increasing or decreasing at a given age). To do so, we can use the `simple_slopes()` function from the [**interactions**]() package. One quirk is that this function does not recognize a polynomial term like the one we have here. Instead, the function looks for separate variable labels. Well fortunately we can just trick it into working as we would like by duplicating the age variable under a new name (`age_temp`). We can then re-estimate our model with the interaction of age and our temporary age variable included (i.e., `age:age_temp`). 

```{r accelerated 05, message = FALSE, warning = FALSE, error = FALSE}
accelerated <- accelerated %>% mutate(age_temp = age)

probe.accel <- lmer(scale(modularity) ~ 1 + age + age:age_temp + (1 | id), 
                    na.action = na.omit,
                    REML = TRUE,
                    data = accelerated)
```

We can then pass the new model object to the `simple_slopes()` function with `age` as our focal predictor and `age_temp` as the moderator (of course in this case, it doesn't matter which variable we put in which position). We can toggle the `jnplot` argument to `TRUE` in order to generate a plot of the linear effect of age as a function of age. This plot will show us where within the quadratic curve, we have significant increases or decreases, and where the linear effect is not significant.

```{r accelerated 06, message = FALSE, warning = FALSE, error = FALSE}
interactions::sim_slopes(probe.accel, 
                         data = accelerated,
                         pred = age, 
                         modx = age_temp, 
                         jnplot = TRUE,
                         jnalpha = 0.05)
```

We can see that from around 19 to 27, the linear effect of age (i.e., the slope of the tangent line) is not significant but is significantly positive before and significantly negative after this age range. We round these numbers because we cannot really think we have such precise measurements to say that linear increases in modularity become non-significant *exactly* at 18.79. However, this sort of plot can help us distinguish between truly quadratic effects (within increases *and* decreases) versus a plateauing of the outcome at later ages.

```{r accelerated 07, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
accelerated <- accelerated %>% select(-age_temp)
```

## Time Coding

One point of frequent confusion when modeling data (nevermind longitudinal models) is the role of centering the predictors for model results/fit/etc. In general, centering predictors does not change the fundamental information contained within the model, although sometimes it is necessary for practical reason (e.g., reducing collinearity between main effects and product terms). In longitudinal models, the main centering concern is where to place the intercept (i.e., where time is coded 0). While many of our parameter estimates will indeed change based on where we choose to estimate the intercept (most notably the...wait for it...intercept, as well as covariances with the intercept).  Here we will demonstrate with the LCM framework since the factor-loading matrix makes what is happening very explicit, but you could replicate these results with any of the other approaches.

First, we can fit the linear model to the single-cohort data we showed above. Here we will place the $0$ factor loading at the first time point which will result in the intercept reflecting individual differences in the level of the outcome at the initial assessment (i.e., initial status).

```{r time coding 01, message = FALSE, warning = FALSE, error = FALSE}
initial.status <- "int =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
                   slp =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4"

initial.status.fit <- growth(initial.status, 
                             data = single.cohort,
                             estimator = "ML",
                             missing="FIML")

summary(initial.status.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

We will focus first on fit and then circle back to parameter estimates later. Here the chi-square test statistics is $12.14$ on $5%$ degrees of freedom ($p = 0.033$), the $CFI = 0.893$ and the $RMSEA = 0.169$. We can then re-estimate the model where we code the *last* factor loading as $0$ instead so the intercept will represent final status.

```{r time coding 02, message = FALSE, warning = FALSE, error = FALSE}
final.status <- "int =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
                 slp =~ -3*dlpfc1 + -2*dlpfc2 + -1*dlpfc3 + 0*dlpfc4"

final.status.fit <- growth(final.status, 
                           data = single.cohort,
                           estimator = "ML",
                           missing = "FIML")

summary(final.status.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

Looking at the model fit values, they are *exactly* identical to the initial status model. We have effectively re-arranged deck chairs on the Titanic (and Rose is about to shove Jack off that door; don't \@ me) as far as model fit goes. So why might we wish to reformulate this model? Well, individual differences in level might be interesting to interpret substantively at one point over another depending on our application of interest. While many research hypotheses will lend themselves naturally to the initial status approach, in intervention work or a training study we might be far more interesting in interpreting where individuals end up (spoken with all the authority of someone who does *not* do intervention work).

```{r time coding 03, message = FALSE, warning = FALSE, error = FALSE, echo=FALSE}
tidy(initial.status.fit) %>% 
  filter(op != "=~", !is.na(statistic)) %>%
  select(-op, -std.nox) %>%
  knitr::kable(label = NA,
               format = "html",
               digits = 3,
               booktabs = TRUE,
               escape = FALSE,
               caption = "**Initial Status LCM**",
               align = "c",
               col.names=c("Parameter", "Estimate", "SE", "Statistic",
                           "*p*-value", "Std.LV", "Std.All"),
               row.names = FALSE) %>% 
  kableExtra::row_spec(row = 0, align = "c")
```


```{r time coding 04, message = FALSE, warning = FALSE, error = FALSE, echo=FALSE}
tidy(final.status.fit) %>% 
  filter(op != "=~", !is.na(statistic)) %>%
  select(-op, -std.nox) %>%
  knitr::kable(label = NA,
               format = "html",
               digits = 3,
               booktabs = TRUE,
               escape = FALSE,
               caption = "**Final Status LCM**",
               align = "c",
               col.names=c("Parameter", "Estimate", "SE", "Statistic",
                           "*p*-value", "Std.LV", "Std.All"),
               row.names = FALSE) %>% 
  kableExtra::row_spec(row = 0, align = "c")
```

We can see that some parameters won't change based on the time-coding. For instance, the residual variances of the repeated measures (and therefore the R-squares), and the mean and variance of the slope factor. However, the mean and variance of the intercept changes, as does the correlation between the slope and intercept (initial status: $r = -0.243$; final status: $r = 0.552$). Note that the sign of the correlation flips and the magnitude of the difference is quite substantial. This should urge caution not to interpret the intercept outside of the specific timepoint it is estimated for since it's parameter values will be context specific (due to rank-order shifts due to random slopes).

If we wanted to use a mixed effect model, we might be better off just transforming our predictor variable to reflect the code we want. We can transform the `single.cohort` data below to suit our purposes.

```{r time coding 05, message = FALSE, warning = FALSE, error = FALSE}
single.cohort.long <- single.cohort %>%
  pivot_longer(cols=starts_with(c("age", "dlpfc")),
               names_to = c(".value", "wave"), 
               names_pattern = "(.+)(.)") %>%
  mutate(wave = as.numeric(wave),
         age_initial = age - min(age, na.rm = TRUE),
         age_final = age - max(age, na.rm = TRUE))
```

We can then fit the models as we usually do to see the equivalencies.

```{r time coding 06, message = FALSE, warning = FALSE, error = FALSE}
initial.status.mlm <- lmer(dlpfc ~ 1 + age_initial + (1 + age_initial | id),
                           na.action = na.omit,
                           REML = TRUE,
                           data = single.cohort.long)

final.status.mlm <- lmer(dlpfc ~ 1 + age_final + (1 + age_final | id),
                         na.action = na.omit,
                         REML = TRUE,
                         data = single.cohort.long)
```


```{r time coding 07, message = FALSE, warning = FALSE, error = FALSE}
tab_model(initial.status.mlm, final.status.mlm,
          show.se = TRUE,
          show.df = FALSE,
          show.ci = FALSE,
          digits = 3,
          pred.labels = c("Intercept", "Age - min(Age)", "Age - max(Age)"),
          dv.labels = c("Initial Status", "Final Status"),
          string.se = "SE",
          string.p = "P-Value")
```

Note that the slope estimates are identical, while estimates involving the intercept change as we saw before.

## Additional Considerations

One thing that has been consistent across all the models that we have fit thus far is that they are structured primarily by the chronological age of the individuals in question. Even when we have used wave of assessment as the nominal variable encoding time, waves have been spaced into yearly assessments and the lack of heterogeneity in ages collapses age and wave (either in reality or in practice when we fix factor loadings). However, nothing stops us from structuring the repeated measures by some other metric of maturation, practice, or anything else (except for pesky things like convention and serious measurement/modeling challenges, but that's all). For instance, we could plot our repeated measures in the `accelerated` data again, but instead of chronological age, we could put pubertal status (measured by Tanner stage) on the x-axis. That would result in the plot below.

### Alternative Metrics of Time
```{r alternative metric 01, message = FALSE, warning = FALSE, error = FALSE}
ggplot(accelerated, aes(x = puberty, y = id)) +
  geom_line(aes(group = id), size = .5) +
  geom_point(aes(color = as.factor(wave))) + 
  labs(x = "Tanner Stage", y = "Individual", color = "Wave", shape = "Cohort") + 
  theme(legend.position = "top")
```

In some ways, this roughly resembles the age plot, with some individuals being measured at earlier stages of puberty and other later. However, close inspection starts to uncover some issues. First, we often stop measuring puberty once individual reach a certain age/stage of development, so puberty as a measure of maturation is limited in it's utility outside of a fairly specific phase of development. We can already see for subjects who are measured first at later ages having missing data for later observations because puberty measures were not taken. There is also the curious case that some subset of individuals seem to move backwards in pubertal development (this should be treated as an error, but it's amusing nonetheless). However, the key feature of interest in a plot like this is that, unlike age, there is not reason to expect that all individuals "age" at the same rate across puberty, seen by the uneven spacing between waves within individuals. This variation is just one of the exciting opportunities that modeling maturation using metric other than age present (although they do come with their own challenges; there is an admittedly understandable reason that age is the predominate variable in longitudinal models). One other advantage is within-age heterogeneity, which we can see by plotting Tanner Stage by age in the plot below.

```{r alternative metric 02, message = FALSE, warning = FALSE, error = FALSE}
ggplot(accelerated, aes(x = age, y = puberty)) +
  geom_line(aes(group = id), size = .5) +
  geom_point(aes(color = as.factor(wave))) + 
  labs(x = "Age", y = "Tanner Stage", color = "Wave", shape = "Cohort") + 
  theme(legend.position = "top")
```

We can see that same-age peers may differ considerable (depending on when we measure them) in their pubertal development. We can also fit puberty-related trajectories to the modularity data below.

```{r alternative metric 03, message = FALSE, warning = FALSE, error = FALSE}
accel <- lmer(scale(modularity) ~ 1 + puberty + (1 | id), 
              na.action = na.omit,
              REML = TRUE,
              data = accelerated)

summary(accel, correlation = FALSE)
```

Of course, we might wish to model age and puberty simultaneously. That may present practical challenges in this data, since age and puberty are highly correlated $r = 0.796$, but some strategies for modeling multiple growth processes have been suggested elsewhere (see the relevant portion of the main text for details).

### Residual Estimates

The final consideration we will explore in our discussion of time is the structure of the residuals of our repeated measures. This is often not of great theoretical importance (I don't really believe you if you tell me your theory is specific enough to know if residual variance should be constant or not...call me a grinch), but it can be really important for model results. Essentially, an overly restrictive assumption about residual variances at the individual observation level can radiate out into the random effects structure and bias your effects. Here we will show how to specify both forms of a model in a mixed effect and structural equation model, and test which one is a better fit to the data.

We can start with the LCM, where heteroscedasticity (i.e., unique residual estimates across) is the default. To obtain this model, we use the same syntax we have seen repeatedly so far.

```{r residuals 01, message = FALSE, warning = FALSE, error = FALSE}
lcm.het <- "int =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
            slp =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4"

lcm.het.fit <- growth(lcm.het, 
                      data = single.cohort,
                      estimator = "ML",
                      missing = "FIML")

summary(lcm.het.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

We can easily see that each residual obtains a unique value with an associated inference test (`.dlpfc1 - .dlpfc4`). To constrain these residuals equal across time, we have to explicitly write out the residual term (we have been relying on sensible defaults thus far) and then pre-multiply each estimate by the same alpha-numeric label. **Lavaan** will interpret repeated labels as a request for an equality constraint on those parameters. We can see this constraint in action below.

```{r residuals 02, message = FALSE, warning = FALSE, error = FALSE}
lcm.hom <- "int =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
            slp =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4

            dlpfc1 ~~ c1*dlpfc1
            dlpfc2 ~~ c1*dlpfc2
            dlpfc3 ~~ c1*dlpfc3
            dlpfc4 ~~ c1*dlpfc4"
```

The choice of constraint (here `c1`) is somewhat arbitrary, but the point is that it can contain characters and numbers (although it must begin with a character). When we fit the model, we will see those parameters will be held equal.

```{r residuals 03, message = FALSE, warning = FALSE, error = FALSE}
lcm.hom.fit <- growth(lcm.hom, 
                      data = single.cohort,
                      estimator = "ML",
                      missing = "FIML")

summary(lcm.hom.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

**Lavaan** conveniently includes the labels in the output and we can see that now all of the residuals obtain the *exact* same value, and *exact* same inferential test (note how that changes the inference on the residual of `dlpfc1` from above). However, note how other parameter estimates (namely the factor means and variances) change as well. These changes are small (for reasons we will see) but it is important to note we are making real changes to the model with the introduction of this constraint. We can then get a likelihood-ratio test of difference in fit, using the `lavTestLRT()` function, because the simplified homoscedastic model is nested within the more complex heteroscedastic model. Note that we list the homoscedastic model first.

```{r residuals 04, message = FALSE, warning = FALSE, error = FALSE}
lavaan::lavTestLRT(lcm.hom.fit, lcm.het.fit)
```

However, in the output, the heteroscedastic model is listed first. This is because the more complex model is always the reference and we are testing whether the imposition of model simplifications (i.e., constraints; here homoscedastic residuals) significantly decreases model fit. Here we can see that the chi-square difference test is not significant ($p = 0.460$), so we could retain the simplification without decreasing the fit of the model to our data. This is the reason that our parameters did not change much. In models where we would reject homoscedasticity, the imposition of the constraint would lead to more pronounced parameter changes elsewhere in the model. While there is not an associated inference test for the AIC/BIC, we could also use those values for model comparisons if we wished. 

To implement heteroscedastic residuals in mixed-effects models, we have to leave our familiar `lmer()` and travel back in time to the days of `lme()`. We can first fit the canonical model we know and love, where the default is homoscedasticity.

```{r residuals 05, message = FALSE, warning = FALSE, error = FALSE}
mlm.hom <- nlme::lme(dlpfc ~ 1 + wave,
                     random = ~ 1 + wave | id,
                     na.action = na.omit,
                     method = "REML",
                     data = single.cohort.long)

summary(mlm.hom, correlation = FALSE)
```

Note that in the `Random effects` portion of the output, we obtain a single numerical value for the residual $\sigma^2 = 0.660$. To fit a heteroscedastic model, we need to make use of a new argument, `weights`. We can specify the `form` argument of the `varIdent()` ("variance identity) function to obtain a unique estimate per `wave`. 

```{r residuals 06, message = FALSE, warning = FALSE, error = FALSE}
mlm.het <- nlme::lme(dlpfc ~ 1 + wave,
                     random = ~ 1 + wave | id,
                     weights = varIdent(form = ~ 1 | wave),
                     na.action = na.omit,
                     method = "REML",
                     data = single.cohort.long)

summary(mlm.het, correlation = FALSE)
```

Looking at the `Random effects` output, we still only see a single value for the residual. Unfortunately the output structure here leaves something to be desired. We need to look to the `Variance function` output section where we see for unique values. Well these are not technically the residual estimates, but rather they indicate the relative size of the residual scaled to the first estimate (the one we see in the `Random effects` section). To extract the actual estimates, we can use the following code.

```{r residuals 07, message = FALSE, warning = FALSE, error = FALSE}
mlm.het$sigma * exp(as.numeric(mlm.het$modelStruct$varStruct))
```

So the first residual can be extracted as `model$sigma`, but the residual weights from the model are stuck in the `varStruct` portion of the fitted `lme()` object and are in the $log(\sigma)$ scale. So we have to extract the values, convert them to numeric format, exponentiate them, and then multiple the weights by the estimated residual to obtain the other three estimates (tired yet?).

Fortunately, model comparisons are straightforward with the `anova()` function, which we can use in the same way we used `lavTestLRT()` to compare LCMs early.

```{r residuals 08, message = FALSE, warning = FALSE, error = FALSE}
anova(mlm.het, mlm.hom)
```

Like before, these results indicate that imposing homoscedasticity does not significantly decrease model fit.

```{r, cleanup time, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```

<!--chapter:end:03-time.Rmd-->

# The Shape of Development {#shape}

However, we define the underlying metric of time to structure our longitudinal model, one of the key substantive questions often underlying work in developmental science is to characterize the course that a given construct takes over time. Here we will highlight many of the different developmental trajectories that we can fit to our data, starting with relatively simple polynomial shapes and working our way up to modeling fully nonlinear trends. In addition to the `feedback.learning` data we have used thus far, we will also use data drawn from the `external-math.csv` and `adversity.csv` files. The `external.math` data contains up to $5$ repeated observations from $405$ children aged $6$ to $14$, measured once every $2$ years. Here we will focus on measures of externalizing behavior and math proficiency. The `adversity` data contains fractional anisotropy (FA) measures from $398$ children measured up to $4$ times across ages $4$ to $11$. We previously used a subset of this data in the [Time](#time) chapter, but here we will utilize the entire sample.

```{r read data shape, message = FALSE, warning = FALSE, error = FALSE}
external.math <- read.csv("data/external-math.csv")

adversity <- read.csv("data/adversity.csv")

feedback.learning <- read.csv("data/feedback-learning.csv") %>% 
  select(id, age, modularity, learning.rate)
```

## Polynomial Trajectories

Like we discussed in the main text, polynomial trajectories are far and away the most common trajectories modeled with longitudinal data. They require relatively few unique timepoints, are straightforward to model, and offer easily-interpretable parameter estimates.

### Intercept-Only Model

We can first consider the simplest polynomial model, one without even a slope. The intercept-only model simply models person-specific differences in average level across time. We can start here with the LCM, which makes the various specifications easiest to see, but we will also build syntax for models in the other frameworks.

```{r polynomials 01, message = FALSE, warning = FALSE, error = FALSE}
int.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14"

int.lcm.fit <- growth(int.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(int.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

We can see that there is significant variance in the intercept factor, suggesting meaningful person-to-person variability in level of externalizing behavior during late childhood and early adolescence. While this might seem a somewhat silly model to fit to these data, this is one half of a random-intercept cross-lag panel model and might be appropriate if we do not expect systematic change over time. However, these intercept-only models are admittedly more plausible for intensive longitudinal data. The MLM specification for this model can be seen below. We will first transform the data into long format before fitting the model.

```{r polynomials 02, message = FALSE, warning = FALSE, error = FALSE}
external.math.long <- external.math %>% 
  pivot_longer(cols = starts_with(c("ext", "math")), 
                      names_to = c(".value", "age"), 
                      names_pattern = "(ext|math)(.+)") %>%
  mutate(age = as.numeric(age))

int.mlm <- lmer(ext ~ 1 + (1 | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long)

summary(int.mlm, correlation = FALSE)
```

Note that this is just a random-effects ANOVA model with no predictors.

### Linear Model

We will move quickly through the linear polynomial models because we have covered them extensively thus far. Below is the syntax for the linear LCM. Remember that assessments are biannual so factor loadings should increase by two for each wave.

```{r polynomials 03, message = FALSE, warning = FALSE, error = FALSE}
lin.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14"

lin.lcm.fit <- growth(lin.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(lin.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

While we have ignored model fit for most models, one nice thing about many of these models, is that they are nested and allow for formal model comparison with likelihood ratio tests, similar to those we saw with hetero- vs. homoscedastic residuals. For instance, we can compare the intercept-only with a linear model.

```{r polynomials 04, message = FALSE, warning = FALSE, error = FALSE}
lavTestLRT(int.lcm.fit, lin.lcm.fit)
```
Remember that we compare whether the more constrained model (here the intercept-only) induces a significant *decrease* in model fit. Here this is true, suggesting that we should retain the linear model over the intercept-only model. If we take a peak at the model fit, this is because the linear model fits the data reasonably well, while the intercept-only model is quite poor in terms of fit.

```{r polynomials 05, message = FALSE, warning = FALSE, error = FALSE}
summary(int.lcm.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)

summary(lin.lcm.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

To fit the corresponding MLM, we first need to generate our linear predictor as an observed variable in our data frame (and will need to do so each time we increase the order of the model). Here we will generate the linear predictor by simply subtracting $6$ from the `age` variable.

```{r polynomials 06, message = FALSE, warning = FALSE, error = FALSE}
external.math.long$age <- external.math.long$age - min(external.math.long$age)

lin.mlm <- lmer(ext ~ 1 + age + (1 + age | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long,
                control = lmerControl(optimizer = "bobyqa",
                                      optCtrl = list(maxfun = 2e5)))

summary(lin.mlm, correlation = FALSE)
```

And the model comparison reveals the same preference for the linear model. Note that the model is re-estimated with ML [FIML] because the two models contain different fixed effects. REML models can be compared when the models differ only in the variance structure. Fortunately this will be done automatically so we don't have to "manually" re-estimate the models.

```{r polynomials 07, message = FALSE, warning = FALSE, error = FALSE}
anova(int.mlm, lin.mlm)
```

Finally, we can see a version of the linear LCSM for the `external.math` data below. Note again that biannual observations mean that we need to set the factor loadings for the slope factor to $2$ instead of $1$ to indicate the spacing appropriately.

```{r polynomials 08, message = FALSE, warning = FALSE, error = FALSE}
lin.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
            pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
            pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
            pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
            pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
            # Regressions Between Adjacent Observations
            pext8 ~ 1*pext6
            pext10 ~ 1*pext8
            pext12 ~ 1*pext10
            pext14 ~ 1*pext12
        
            # Define Change Latent Variables (delta)
            delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
            delta32 =~ 1*pext10; delta32 ~~ 0*delta32
            delta43 =~ 1*pext12; delta43 ~~ 0*delta43
            delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
            # Define Intercept and Slope
            int =~ 1*pext6
            slp =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
        
            int ~ 1
            slp ~ 1
            
            int ~~ slp
            slp ~~ slp
"

lin.lcsm.fit <- sem(lin.lcsm, 
                    data = external.math, 
                    estimator = "ML",
                    missing = "FIML")

summary(lin.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

### Quadratic Model 

Next, we can add an additional factor to capture quadratic curvature in our data. Below is the LCM syntax for this model. Note that this is an extremely straightforward expansion of the syntax we have seen thus far. While this won't be the case here, sometimes we need to worry about numerically large factor loadings causing some estimation issues in practice (nothing theoretical is wrong with large factor loadings). In those instances, we could divide our factor loadings by some constant to control those values from getting to large (although this will change the interpretation of a per-unit change).

```{r polynomials 09, message = FALSE, warning = FALSE, error = FALSE}
quad.lcm <- "int  =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
             slp  =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
             quad =~ 0*ext6 + 4*ext8 + 16*ext10 + 36*ext12 + 64*ext14"

quad.lcm.fit <- growth(quad.lcm, 
                       data = external.math,
                       estimator = "ML",
                       missing = "FIML")

summary(quad.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcm.fit, quad.lcm.fit)
```

The MLM syntax is similarly straightforward. To add a powered term, we can use the `I()` function, or the `poly()` function if we wished to use orthogonal polynomials.

```{r polynomials 10, message = FALSE, warning = FALSE, error = FALSE}
quad.mlm <- lmer(ext ~ 1 + age + I(age^2) + (1 + age + I(age^2) | id),
                 na.action = na.omit,
                 REML = TRUE,
                 data = external.math.long,
                 control = lmerControl(optimizer = "bobyqa",
                                       optCtrl = list(maxfun = 2e5)))

summary(quad.mlm, correlation = FALSE)
```

While these models converge without too much issue here, note the strong correlation between the linear and quadratic terms, suggesting that the quadratic term is largely redundant. This is often the case with low numbers of repeated measures. We can technically fit some non-linearities, but they may not be particularly well-specified.

The LCSM syntax requires a bit more explanation. The quadratic model is shown below.

```{r polynomials 11, message = FALSE, warning = FALSE, error = FALSE}
quad.lcsm <- "
             # Define Phantom Variables (p = phantom)
             pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
             pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
             pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
             pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
             pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
             # Regressions Between Adjacent Observations
             pext8 ~ 1*pext6
             pext10 ~ 1*pext8
             pext12 ~ 1*pext10
             pext14 ~ 1*pext12
        
             # Define Change Latent Variables (delta)
             delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
             delta32 =~ 1*pext10; delta32 ~~ 0*delta32
             delta43 =~ 1*pext12; delta43 ~~ 0*delta43
             delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
             # Define Intercept and Slope
             int  =~ 1*pext6
             slp  =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
             quad =~ 4*delta21 + 12*delta32 + 20*delta43 + 28*delta54 
        
             int  ~ 1
             slp  ~ 1
             quad ~ 1
            
             int ~~ slp
             int ~~ quad
             slp ~~ slp
             slp ~~ quad
             quad ~~ quad
"
```

We talked in the [Canonical](#canon) chapter about how the loadings for the linear factor were all $1$, and this could be thought of as summing across the difference factors. Another way to think of this specification is that the factor loadings for the LCSM are the *differences* between successive loadings for the LCM. In the standard linear case, these are all $1$s to indicate a constant effect across units of time, whereas in our example in this chapter, they are all differences of $2$ to reflect biannual observations. The same principle can be applied to the loadings for higher-order factors in the LCSM. For a quadratic factor, the LCM loadings are [$0$, $4$, $16$, $36$, $64$], and therefore the LCSM loadings should be [($4 - 0$), ($16 - 4$), ($36 - 16$), ($64 - 36$)] = [$4$, $12$, $20$, $28$]. As a sanity check, we can fit this model and the parameter estimates should match the LCM results exactly.

```{r polynomials 12, message = FALSE, warning = FALSE, error = FALSE}
quad.lcsm.fit <- sem(quad.lcsm,
                     data = external.math, 
                     estimator = "ML",
                     missing = "FIML")

summary(quad.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcsm.fit, quad.lcsm.fit)
```

### Inverse Model

The final polynomial model we will consider is the inverse model. Unlike the quadratic curvature which reverses, the inverse curve approaches a plateau asymptotically. We can do a quick algebraic transformation to make the factor loadings tractable by inverting the original (i.e., not centered) values and subtracting them from $1$. So for linear loadings [$1$, $3$, $5$, $7$, $9$], we would have inverse factor loadings [$1 - (1/1)$, $1 - (1/3)$, $1 - (1/5)$, $1 - (1/7)$, $1 - (1/9)$] or [$0$, $2/3$, $4/5$, $6/7$, $8/9$]. Inverting the original loadings avoids trying to take the reciprocal of 0 (which results in 6 more weeks of COVID variants) and subtracting from one specifies an upper rather than lower bound effect (this won't change the nature of the effect, it just makes the sign easier to interpret).

```{r polynomials 13, message = FALSE, warning = FALSE, error = FALSE}
inv.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
            inv =~ 0*ext6 + (2/3)*ext8 + (4/5)*ext10 + (6/7)*ext12 + (7/8)*ext14"

inv.lcm.fit <- growth(inv.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(inv.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcm.fit, inv.lcm.fit)
```

Note that we are comparing the inverse and linear models, not the inverse and quadratic. This is because while the linear model is nested within both quadratic and inverse models, the two are not nested with respect to one another. However, we might graphically examine the trends implied by the model for a moment.

```{r polynomials 14, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
ggplot(data.frame(id=quad.lcm.fit@Data@case.idx[[1]], 
                           lavPredict(quad.lcm.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("ext"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(ext)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = ext, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Quadratic LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 

ggplot(data.frame(id=inv.lcm.fit@Data@case.idx[[1]], 
                           lavPredict(inv.lcm.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("ext"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(ext)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = ext, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Inverse LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 
```

What should be visually apparent is that we get quite a few flips in the direction of curvature in the inverse compared to the quadratic model. Indeed the quadratic effect is negative ($-0.015$) and the inverse effect is positive ($0.513$). This sensitivity is likely another indication that curvature is really over-fitting noise in these data rather than reflecting some true non-linearity. Below are how to achieve this model with the MLM:

```{r polynomials 15, message = FALSE, warning = FALSE, error = FALSE}
external.math.long$age_inv <- 1 - (external.math.long$age + 1)^(-1)

inv.mlm <- lmer(ext ~ 1 + age + age_inv + (1 + age + age_inv | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long,
                control = lmerControl(optimizer = "bobyqa",
                                      optCtrl = list(maxfun = 2e5)))

summary(inv.mlm, correlation = FALSE)

anova(lin.mlm, inv.mlm)
```

and LCSM (note that the subtraction method gets a little messy for the slope loadings):

```{r polynomials 16, message = FALSE, warning = FALSE, error = FALSE}
inv.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
            pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
            pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
            pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
            pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
            # Regressions Between Adjacent Observations
            pext8 ~ 1*pext6
            pext10 ~ 1*pext8
            pext12 ~ 1*pext10
            pext14 ~ 1*pext12
        
            # Define Change Latent Variables (delta)
            delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
            delta32 =~ 1*pext10; delta32 ~~ 0*delta32
            delta43 =~ 1*pext12; delta43 ~~ 0*delta43
            delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
            # Define Intercept and Slope
            int =~ 1*pext6
            slp =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
            inv =~ (2/3)*delta21 + (2/15)*delta32 + (2/35)*delta43 + (1/56)*delta54 
        
            int ~ 1
            slp ~ 1
            inv ~ 1
            
            int ~~ slp
            int ~~ inv
            slp ~~ slp
            slp ~~ inv
            inv ~~ inv
"

inv.lcsm.fit <- sem(inv.lcsm,
                    data = external.math, 
                    estimator = "ML",
                    missing = "FIML")

summary(inv.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcsm.fit, inv.lcsm.fit)
```

## Piecewise Trajectories

If we do not think that a single polynomial function can sufficiently capture a complex trajectory, we might consider bolting two (or more) polynomial functions together using a piecewise approach. Here we will use the `adversity` data which covers $8$ years of childhood (ages $4-11$). The simplest piecewise trajectory can be constructed two distinct linear pieces joined at a knot point. We need at least 3 time points to specify a line but the pieces can share a time point at the knot point. This means we need a minimum of $5$ time points in order to fit even the simplest piecewise model. Note that with this minimum, the knot point is constrained to be at the middle time point, and the knot can never be placed at the first or last two time points because of the 3 time point requirement to estimate the linear slope. Note that as we discussed before, these time point requirements can be accomodated at the group level, and no one individual need be observed $5$ or more times. Indeed this is the case here, where no individual is measured more than $4$ times.

There are two general approaches for specifying piecewise models. The first, and more common, approach is the two-rate specification, where each effect can be interpreted in isolation like a regular linear model. We  specify the two-rate LCM using the syntax below. Note that we code the factor loadings in such a way that the intercept is at the knot point (age 8).

```{r piecewise 01, message = FALSE, warning = FALSE, error = FALSE}
two.rate <- "int  =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                     1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
             slp1 =~ -3*fmin4 + -2*fmin5 + -1*fmin6 + 0*fmin7 + 
                     0*fmin8 + 0*fmin9 + 0*fmin10 + 0*fmin11
             slp2 =~ 0*fmin4 + 0*fmin5 + 0*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
"

two.rate.fit <- growth(two.rate,
                       data = adversity,
                       estimator = "ML",
                       missing = "FIML")

summary(two.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)

ggplot(data.frame(id=two.rate.fit@Data@case.idx[[1]], 
                           lavPredict(two.rate.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("fmin"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(fmin)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = fmin, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "2-Rate Piecewise LCM Trajectories",
       x = "Age",
       y = "Predicted Forceps Minor Microstructure") +
  theme(legend.position = "none") 
```

The second approach is the added-rate approach where the second slope is defined as the deflection from the original trajectory. We can see this approach below.

```{r piecewise 02, message = FALSE, warning = FALSE, error = FALSE}
add.rate <- "int  =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                     1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
             slp1 =~ -3*fmin4 + -2*fmin5 + -1*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
             slp2 =~ 0*fmin4 + 0*fmin5 + 0*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
"

add.rate.fit <- growth(add.rate,
                       data = adversity,
                       estimator = "ML",
                       missing = "FIML")

summary(add.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)
```

Note that whichever approach we take, the models fit the data identically. This means that the choice between these two specifications should be guided by theoretical considerations of which type of effect you would prefer to interpret.

Of course, the models we have seen thus far in this section are the simplest linear-linear piecewise functions we can specify. We could even better specify these linear pieces with additional time points, or we could potentially increase the polynomial order of one or more of the pieces. This could be a great way to model nonlinear growth followed by a plateau for instance. Below, we demonstrate a quadratic-linear piecewise function that could capture this type of growth pattern. To identify this model, we can use trial-level data from the `feedback.learning` dataset source, with 4 trials specifying the quadratic initial piece, and the remaining trials specifying the second linear slope. Below we show the code to fit and visualize this model.

```{r piecewise 03, message = FALSE, warning = FALSE, error = FALSE}
trials <- read.csv("data/trials.csv")

quad.rate <- "int =~ 1*trial.1 + 1*trial.2 + 1*trial.3 + 1*trial.4 + 
                     1*trial.5 + 1*trial.6 + 1*trial.7
             slp1 =~ -3*trial.1 + -2*trial.2 + -1*trial.3 + 0*trial.4 + 
                     0*trial.5 + 0*trial.6 + 0*trial.7
             quad =~ 9*trial.1 + 4*trial.2 + 1*trial.3 + 0*trial.4 + 
                     0*trial.5 + 0*trial.6 + 0*trial.7        
             slp2 =~ 0*trial.1 + 0*trial.2 + 0*trial.3 + 0*trial.4 + 
                     1*trial.5 + 2*trial.6 + 3*trial.7
             
"

quad.rate.fit <- growth(quad.rate,
                       data = trials,
                       estimator = "ML",
                       missing = "FIML")

summary(quad.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)

ggplot(data.frame(id=quad.rate.fit@Data@case.idx[[1]], 
                           lavPredict(quad.rate.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("trial"), 
                               names_to = c(".value", "num"), 
                               names_pattern = "(trial).(.)") %>%
                  dplyr::mutate(num = as.numeric(num)), 
                aes(x = num, 
                    y = trial, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Quadradic-Linear Piecewise LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 

```

To fit piecewise models in the MLM, we simply create observed variables that correspond to the factor loadings we specified in the LCM code. Below we show how to generate these observed varaiables and the syntax to fit the two-rate version of the linear-linear model.

```{r piecewise 04, message = FALSE, warning = FALSE, error = FALSE}
adversity.long <- adversity %>% 
  pivot_longer(cols = starts_with("fmin"), 
               names_to = c(".value", "age"), 
               names_pattern = "(fmin)(.+)") %>%
  mutate(age = as.numeric(age),
         slp1 = ifelse(age > 7, 0, age - 7),
         slp2 = ifelse(age < 7, 0, age - 7),
         quad = ifelse(age > 7, 0, (age - 7)^2))

two.rate.mlm <- lmer(fmin ~ 1 + slp1 + slp2 + (1 + slp1 + slp2 | id),
                     na.action = na.omit,
                     REML = TRUE,
                     data = adversity.long,
                     control = lmerControl(optimizer = "bobyqa",
                                           optCtrl = list(maxfun = 2e5)))
summary(two.rate.mlm)
```

Fitting the piecewise model in the LCSM framework requires some additional work and unless we wish to include the dual-change effects, these models could be more-simply implemented as LCMs. This is due to a general complication in LCSMs if we wish to place the intercept anywhere except at the initial timepoint. If we look below at the `Regressions Between Adjacent Observations` section of the code below, we can see that for timepoints that come before the intercept (here before the knot point) we actually regression *earlier* timepoints on *later* timepoints. This reversal of the intuitive temporal order allows the LCSM to mimic the negative factor loadings in the LCM. Note also that the intercept timepoint (here `pfmin7`) appears twice, predicting the timepoint both before and after it. Finally, the loadings of the timepoints prior to the intercept load at a $-1$ on the delta factors rather than a $1$ as usual. The remainder of the model follows the conventions we are accustomed, but we comment below on the specific changes needed here.

```{r piecewise 05, message = FALSE, warning = FALSE, error = FALSE}
two.rate.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pfmin4 =~ 1*fmin4; fmin4 ~ 0; fmin4 ~~ fmin4; pfmin4 ~~ 0*fmin4
            pfmin5 =~ 1*fmin5; fmin5 ~ 0; fmin5 ~~ fmin5; pfmin5 ~~ 0*fmin5
            pfmin6 =~ 1*fmin6; fmin6 ~ 0; fmin6 ~~ fmin6; pfmin6 ~~ 0*fmin6
            pfmin7 =~ 1*fmin7; fmin7 ~ 0; fmin7 ~~ fmin7; pfmin7 ~~ 0*fmin7
            pfmin8 =~ 1*fmin8; fmin8 ~ 0; fmin8 ~~ fmin8; pfmin8 ~~ 0*fmin8
            pfmin9 =~ 1*fmin9; fmin9 ~ 0; fmin9 ~~ fmin9; pfmin9 ~~ 0*fmin9
            pfmin10 =~ 1*fmin10; fmin10 ~ 0; fmin10 ~~ fmin10; pfmin10 ~~ 0*fmin10
            pfmin11 =~ 1*fmin11; fmin11 ~ 0; fmin11 ~~ fmin11; pfmin11 ~~ 0*fmin11
            
            # Regressions Between Adjacent Observations
            pfmin4 ~ 1*pfmin5  # temporal order reversed before intercept
            pfmin5 ~ 1*pfmin6
            pfmin6 ~ 1*pfmin7
            pfmin8 ~ 1*pfmin7  # intercept time point appears twice
            pfmin9 ~ 1*pfmin8
            pfmin10 ~ 1*pfmin9
            pfmin11 ~ 1*pfmin10
            
            # Define Change Latent Variables (delta)
            # loadings prior to the intercept are negative
            delta21 =~ -1*pfmin4;  delta21 ~~ 0*delta21 
            delta32 =~ -1*pfmin5;  delta32 ~~ 0*delta32
            delta43 =~ -1*pfmin6;  delta43 ~~ 0*delta43
            
            # loadings after the intercept are as usual
            delta54 =~ 1*pfmin8;  delta54 ~~ 0*delta54
            delta65 =~ 1*pfmin9;  delta65 ~~ 0*delta65
            delta76 =~ 1*pfmin10;  delta76 ~~ 0*delta76
            delta87 =~ 1*pfmin11;  delta87 ~~ 0*delta87
            
            # Define Intercept and Slope
            int  =~ 1*pfmin7
            slp1 =~ 1*delta21 + 1*delta32 + 1*delta43
            slp2 =~ 1*delta54 + 1*delta65 + 1*delta76 + 1*delta87
        
            int ~ 1; slp1 ~ 1; slp2 ~ 1
            
            slp1 ~~ slp1
            slp2 ~~ slp2
            int ~~ slp1 + slp2
            slp1 ~~ slp2
"
two.rate.lcsm.fit <- sem(two.rate.lcsm,
                         data = adversity, 
                         estimator = "ML",
                         missing = "FIML")

summary(two.rate.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

If we wish to include proportional change into this model, there is another additional oddity. For the beta parameters prior to the intercept, we actually create a cycle in our model (in technical terms the model is non-recursive), where the delta factor that a given phantom loads on is then regressed on that same phantom variable. The model remains identified, however, because the loading is fixed to $-1$ while the proportional effect is freely estimated. The relevant syntax is below.

```{r piecewise 06, message = FALSE, warning = FALSE, error = FALSE}
two.rate.prop.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pfmin4 =~ 1*fmin4; fmin4 ~ 0; fmin4 ~~ fmin4; pfmin4 ~~ 0*fmin4
            pfmin5 =~ 1*fmin5; fmin5 ~ 0; fmin5 ~~ fmin5; pfmin5 ~~ 0*fmin5
            pfmin6 =~ 1*fmin6; fmin6 ~ 0; fmin6 ~~ fmin6; pfmin6 ~~ 0*fmin6
            pfmin7 =~ 1*fmin7; fmin7 ~ 0; fmin7 ~~ fmin7; pfmin7 ~~ 0*fmin7
            pfmin8 =~ 1*fmin8; fmin8 ~ 0; fmin8 ~~ fmin8; pfmin8 ~~ 0*fmin8
            pfmin9 =~ 1*fmin9; fmin9 ~ 0; fmin9 ~~ fmin9; pfmin9 ~~ 0*fmin9
            pfmin10 =~ 1*fmin10; fmin10 ~ 0; fmin10 ~~ fmin10; pfmin10 ~~ 0*fmin10
            pfmin11 =~ 1*fmin11; fmin11 ~ 0; fmin11 ~~ fmin11; pfmin11 ~~ 0*fmin11
            
            # Regressions Between Adjacent Observations
            pfmin4 ~ 1*pfmin5  # temporal order reversed before intercept
            pfmin5 ~ 1*pfmin6
            pfmin6 ~ 1*pfmin7
            pfmin8 ~ 1*pfmin7  # intercept time point appears twice
            pfmin9 ~ 1*pfmin8
            pfmin10 ~ 1*pfmin9
            pfmin11 ~ 1*pfmin10
            
            # Define Change Latent Variables (delta)
            # loadings prior to the intercept are negative
            delta21 =~ -1*pfmin4;  delta21 ~~ 0*delta21 
            delta32 =~ -1*pfmin5;  delta32 ~~ 0*delta32
            delta43 =~ -1*pfmin6;  delta43 ~~ 0*delta43
            
            # loadings after the intercept are as usual
            delta54 =~ 1*pfmin8;  delta54 ~~ 0*delta54
            delta65 =~ 1*pfmin9;  delta65 ~~ 0*delta65
            delta76 =~ 1*pfmin10;  delta76 ~~ 0*delta76
            delta87 =~ 1*pfmin11;  delta87 ~~ 0*delta87
            
            # Define Proportional Change Regressions (beta = equality constraint)
            # Non-recursive Proportional Paths
            delta21 ~ beta*pfmin4
            delta32 ~ beta*pfmin5
            delta43 ~ beta*pfmin6
            
            # Standard Proportional Paths
            delta54 ~ beta*pfmin7
            delta65 ~ beta*pfmin8
            delta76 ~ beta*pfmin9
            delta87 ~ beta*pfmin10
            
            # Define Intercept and Slope
            int  =~ 1*pfmin7
            slp1 =~ 1*delta21 + 1*delta32 + 1*delta43
            slp2 =~ 1*delta54 + 1*delta65 + 1*delta76 + 1*delta87
        
            int ~ 1; slp1 ~ 1; slp2 ~ 1
            
            slp1 ~~ slp1
            slp2 ~~ slp2
            int ~~ slp1 + slp2
            slp1 ~~ slp2
"
two.rate.prop.lcsm.fit <- sem(two.rate.prop.lcsm,
                              data = adversity, 
                              estimator = "ML",
                              missing = "FIML")

summary(two.rate.prop.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

## Nonlinear Trajectories

However, we sometimes need to move beyond well-defined polynomial models in order to capture the complexity of developmental change over time and consider truly nonlinear trajectories. This is most common in applications with dense observations within-person or that cover wide (i.e., decades) age-ranges between-person, although we can model true nonlinearities in standard applications with 4-6 timepoints. GAMMs will shine in the former case, while LCMs and LCSMs offer some options for the latter.

We can start with the GAMM syntax we are familiar with to specify a nonlinear trend across ages $8-28$ in the `feedback.learning` data.

```{r nonlinear 01, message = FALSE, warning = FALSE, error = FALSE, results='asis'}
gamm <- gamm4(scale(modularity) ~ 1 + s(age),
              random = ~ (1 | id),
              data = feedback.learning)

gamtabs(gamm$gam, type = "html",
        pnames = c("Intercept"), snames = c("s(Age)"),
        caption = "Modularity as a Function of Age")

plot.gam(gamm$gam, se = TRUE, rug = TRUE, shade = TRUE,
         xlab = "Age", ylab = "Fitted Modularity Values")
```

Nothing as changed about this model from what we have seen previously because the GAMM intrinsically captures non-linearities through the use of the splines. In principle, given the functional form that is revealed, these data might also be fit with a quadratic polynomial (indeed in McCormick et al., 2021, *NeuroImage*: see main analyses versus supplemental GAMMs as a sensitivity check). Indeed these models might be a nice tool for this purpose to check the reasonableness of the functional form specified in more restrictive polynomial models, even if we retain the polynomials for interpretability. With expanding age ranges, this type of sensitivity check becomes increasingly important, since the non-linearities of the GAMM are a better theoretical match for the complex patterns of growth across the lifespan. While deceptively simple in their implementation (indeed this is the exact same model we keep fitting), GAMMS are a powerful and flexible tool for fitting developmental trajectories.

The SEMs also allow for the inclusion of some nonlinear terms. Like the GAMM, we have already seen the most common nonlinearity through the proportional change parameter of the LCSM. We include the code and output for this model below but otherwise will not explore it further here.

```{r nonlinear 02, message = FALSE, warning = FALSE, error = FALSE}
executive.function <- read.csv("data/executive-function.csv", header = TRUE) %>%
  select(id, dlpfc1:dlpfc4)

proportional.lcsm <- "
               # Define Phantom Variables (p = phantom)
               pdlpfc1 =~ 1*dlpfc1; dlpfc1 ~ 0; dlpfc1 ~~ dlpfc1; pdlpfc1 ~~ 0*pdlpfc1
               pdlpfc2 =~ 1*dlpfc2; dlpfc2 ~ 0; dlpfc2 ~~ dlpfc2; pdlpfc2 ~~ 0*pdlpfc2
               pdlpfc3 =~ 1*dlpfc3; dlpfc3 ~ 0; dlpfc3 ~~ dlpfc3; pdlpfc3 ~~ 0*pdlpfc3
               pdlpfc4 =~ 1*dlpfc4; dlpfc4 ~ 0; dlpfc4 ~~ dlpfc4; pdlpfc4 ~~ 0*pdlpfc4
        
               # Regressions Between Adjacent Observations
               pdlpfc2 ~ 1*pdlpfc1
               pdlpfc3 ~ 1*pdlpfc2
               pdlpfc4 ~ 1*pdlpfc3
        
               # Define Change Latent Variables (delta)
               delta21 =~ 1*pdlpfc2; delta21 ~~ 0*delta21
               delta32 =~ 1*pdlpfc3; delta32 ~~ 0*delta32
               delta43 =~ 1*pdlpfc4; delta43 ~~ 0*delta43
               
               # Define Proportional Change Regressions (beta = equality constraint)
               delta21 ~ beta*pdlpfc1
               delta32 ~ beta*pdlpfc2
               delta43 ~ beta*pdlpfc3
        
               # Define Intercept and Slope
               int =~ 1*pdlpfc1
               slp =~ 1*delta21 + 1*delta32 + 1*delta43
        
               int ~ 1
               slp ~ 1
               
               int ~~ slp
               slp ~~ slp
"

lcsm.proportional <- sem(proportional.lcsm, 
                         data = executive.function, 
                         estimator = "ML",
                         missing = "FIML")

summary(lcsm.proportional)
```

However, the LCM offers an unique opportunity to model non-linear trends by throwing back to its roots in confirmatory factor analysis. Instead of specifying factor loadings, as we have done up to this point, we can allow a subset of them to be freely-estimated. This means that equal change is estimated between each timepoint, growth can accelerate and decelerate across different increments. However, to identify the growth model, we have to set at least two of the factor loadings to pre-specified values ($0$ and $1$) to set the scale for the other factor loadings. In general, there are two reasonable specifications. In the first, we set the first loading to $0$ and the second to $1$. This means that the other factor loadings are proportional to the amount of change that occurs between the first and second time point. We can see this specification below.

```{r nonlinear 03, message = FALSE, warning = FALSE, error = FALSE}
free.load1 <- "int   =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                        1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
               basis =~ 0*fmin4 + 1*fmin5 + l3*fmin6 + l4*fmin7 + 
                        l5*fmin8 + l6*fmin9 + l7*fmin10 + l8*fmin11
"

free.load1.fit <- growth(free.load1,
                         data = adversity,
                         estimator = "ML",
                         missing = "FIML")

summary(free.load1.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)
```

Here we label the freely-estimated factor loadings for convenient reference. We also specify the second factor as a basis (or sometimes "shape") rather than a "slope" factor to reflect the non-linearity inherent in this model. When we examine the model results, we can see a pretty striking pattern of factor loadings. The third loading (`l3`) is $10.43$, which suggests that more than $9$ times the amount of change occurs between the second and third timepoints as does between the first two (remember we take the difference between adjacent loadings). However, `l4` suggests that then there is a sharp deceleration in growth between the next timepoints. At `l6`, we can see that the factor loading actuall *decreases*, which is how this model fits a negative trend between adjacent timepoints. Finally, `l8` suggests a large increase at the final timepoint, reversing the earlier decreases. One thing to highlight is the increase in complexity just in describing the factor loadings and interpreting adjacent timepoints that do not follow some monotonic trend. Like many non-linear models, visualization of the model-implied trajectories is key for interpreting the effects. We can do so below.

```{r nonlinear 04, message = FALSE, warning = FALSE, error = FALSE}
ggplot(data.frame(id=free.load1.fit@Data@case.idx[[1]], 
                           lavPredict(free.load1.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("fmin"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(fmin)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = fmin, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Free-loading Model 1 Trajectories",
       x = "Age",
       y = "Predicted Forceps Minor Microstructure") +
  theme(legend.position = "none") 
```

Here we can see the power, and danger, of these free-loading model. Like a GAMM, we can have complex, localized change, with reversals and highly variable slopes. However, in looking at the first couple of timepoints, we have to wonder if we are overfitting noise in the data when some simpler functional form would (e.g., a line) would describe this trend in a more replicable/generalizable way. Very often, these free-loading models will fit the data better than many alternatives we have discussed elsewhere in this primer, but then be very sample-depenedent in an unsatisfying fashion. Like with the GAMM, one potential use for this model is as a sensitivity check for a more-constrained LCM to ensure we are not completely botching the functional form. 

We can refit this model using a different specification where we set the first loading to $0$ and the *last* loading to $1$. This scales the estimated loadings to be proportions of the total growth realized by that point. We can see this model below.

```{r nonlinear 05, message = FALSE, warning = FALSE, error = FALSE}
free.load2 <- "int   =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                        1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
               basis =~ 0*fmin4 + l2*fmin5 + l3*fmin6 + l4*fmin7 + 
                        l5*fmin8 + l6*fmin9 + l7*fmin10 + 1*fmin11
"

free.load2.fit <- growth(free.load2,
                         data = adversity,
                         estimator = "ML",
                         missing = "FIML")

summary(free.load2.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)
```

We can see that the loadings are now all less than $1$ but that they follow the same pattern as in the model (which they should, as they are identically-fitting models). However, now the loadings are expressed as percentages of the total change that occurs between the first and final timepoint. Note one odd thing is that when a trajectory follows a parabolic shape, we can get loadings about $1$ reflecting $>100%$ of the overall change has occurred by that time point. Nothing is wrong, per se, with that interpretation, but given the oddness of language it evokes, the first specification is by far the more common.

## Fixed and Random Effects

Before we move on, we want to highlight the distinction we make between fixed and random effects when we talk about the limitations of the functional form with respect to the number of repeated measures, per person. The oft-repeated mantra is that you need $3$ timepoints for a line, $4$ for a quadratic, and so on ad infinitum. *But*, on the other hand, we can fit a highly complex non-linear developmental trajectory to data where each individual contributes **at most** $2$ timepoints. How do we reconcile these two things? Well it has to do with whether we treat the effect in question as fixed or random. When we treat a parameter as fixed, we obtain a single value that describes all indiviudals in our sample (barring pesky things like interactions), but when we treat an effect as random, we also model individual differences in that "fixed" parameter (note that we estimate a single variance per random effect, not individual effects, but that's a longer discussion and we can calculate individual effects with some well-developed methods). So to estimate these individually-varying random effects, we need more repeated measures *within-person*. However, we can still estimate an overall fixed effect trajectory that takes advantage of greater timepoints *between* individuals than exist for any given individual. This is how we might estimate a GAMM with some highly complex surface, even though we could never estimate a random effect for any individual since they only contribute up to two observations.

We can demonstrate this with our `feedback.learning` data where we can model both the effect of `wave`, which is a fully within-person measure, as well as linear and quadratic `age`. These latter two effects are specified as fixed effects because while `age` between-person ranges from $8 - 29$, no one person contributes more than $3$ time points, so the quadratic random effect would not be estimable. However, we can estimate a random effect of `wave`. We can see these effects below. This distinction highlights some of the advantages of having measurement timing heterogeneity for estimating more complex effect than one could do with single-cohort data with fully consistent assessment schedules.

```{r fixedrandom 01, message = FALSE, warning = FALSE, error = FALSE}
feedback.learning <- read.csv("data/feedback-learning.csv") %>% 
  select(id, wave, age, modularity, learning.rate)

fixrand <- lmer(scale(modularity) ~ 1 + wave + age + I(age^2) +
                  (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = feedback.learning)

summary(fixrand, correlation = FALSE)
```


```{r, cleanup shape, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```

<!--chapter:end:04-shape.Rmd-->

# Covariates and Distal Outcomes {#covariates}

Thus far, we have only considered *unconditional* growth models. This is somewhat of a misnomer because clearly time is included as a predictor, but that is considered integral to the growth model so we tend not to count it as a conditional model (as with many terminological things in quant: **shrugs**). What we will do in this section is bring in additional variables to our growth model as predictors. These covariates will either predict the parameters of the growth process (i.e., intercept and slope) or the individual repeated measures directly (some models will allow a single covariate to do both simultaneously). We will read in the relevant data sets below.

```{r read data covariates, message = FALSE, warning = FALSE, error = FALSE}
adversity <- read.csv("data/adversity.csv", header = TRUE)

executive.function <- read.csv("data/executive-function.csv", header = TRUE)
```

## Covariates

### Time Invariant Covariates

We will begin with the relatively straightforward time-invariant covariate (TIC). These covariates operate at the level of the growth process: meaning that they predict the random effects (MEMs) or latent factors (SEMs). As the name implies, we will have a single value of each TIC per individual. While we often obtain that value at the first observation, in theory we could have measured that variable at any time and we should get the same value (otherwise it isn't time-invariant, is it?). One common misconception is that there is any temporal ordering inherent at this level of the model. The TIC and the intercept/slope parameters are time-invariant and so we cannot establish temporal precedence unless we bring other knowledge of the data to bear (e.g., the TIC measures something early in life and the growth parameters are fit to adolescent data). This is the same reason why regressing the slope of one outcome on the intercept of the other is very theoretically dubious unless the growth processes are truly separated in time. At this level, we essentially have cross-sectional regressions (again unless we know something extra about our variables temporally). As such, inclusion of TICs is relatively simple compared with what we will consider for the remainder of this section. We will demonstrate our examples in the MLM and LCM because of the relatively simple syntax, but the principles extend naturally to the GAMM and LCSM.

Below we can expand our unconditional growth of `fmin` to include an early life experience variable. Here we will begin with the multilevel model. Including a TIC is trivially easy from a model syntax standpoint. We can add our early life experience variable (`neglect`) into the fixed effect just as we have done for `age`. Even though these variables operate at different levels of the model (`age` is a level 1 variable and `neglect` is a level 2 variable) there is no need to differentiate them in the model syntax. We will need to keep track of these different levels of effect in our *interpretation* but the model really operates at a single **reduced-form** level and our syntax reflects that fact.

```{r tic 01, message = FALSE, warning = FALSE, error = FALSE}
adversity.long <- adversity %>% 
  pivot_longer(cols = starts_with("fmin"), 
               names_to = c(".value", "age"), 
               names_pattern = "(fmin)(.+)") %>% 
  mutate(age = as.numeric(age)) %>%
  drop_na(fmin)

tic.mlm <- lmer(fmin ~ 1 + age + neglect + (1 + age | id),
                     na.action = na.omit,
                     REML = TRUE,
                     data = adversity.long,
                     control = lmerControl(optimizer = "bobyqa",
                                           optCtrl = list(maxfun = 2e5)))

summary(tic.mlm, correlation = FALSE)
```

Here we can see that there is a positive effect of `age`, reflecting general increases in FA values across age. Conversely, there is a negative *between-person* effect of `neglect`, with those experiencing greater levels of early childhood neglect showing reduced levels of white matter FA in the forceps minor (`fmin`). Note that this is a prediction of level, not of slopes. We will return to this in a bit when we consider cross-level interactions.

The corresponding model in the LCM is similarly straightforward.

```{r tic 02, message = FALSE, warning = FALSE, error = FALSE}
tic.lcm <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                   1*fmin9 + 1*fmin10 + 1*fmin11
            slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                   5*fmin9 + 6*fmin10 + 7*fmin11

            int ~ neglect"

tic.lcm.fit <- growth(tic.lcm, 
                      data = adversity,
                      estimator = "ML",
                      missing = "FIML")

summary(tic.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

This syntax highlights the idea that our TIC is only predicting the intercept (i.e., level). We will extend this model later. However, there are two potential extensions of the TIC model in the SEM that we can consider here. One concerns missing data on our exogenous TIC. For **reasons** (the reasons aren't particularly important for our purposes), the standard estimator for SEMs in most software is a conditional estimator, which is the same used in MLMs. The upshot of this estimator is that we cannot accomodate *any* missing data on exogenous variables (i.e, variables that only act as predictors) in the model. With this estimator any individual with missing observations on the exogenous variable will simply be dropped from the model. We can see this below where we predict the intercept by maternal warmth in early childhood (`warmth`).

```{r tic 03, message = FALSE, warning = FALSE, error = FALSE}
joint.lik <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                     1*fmin9 + 1*fmin10 + 1*fmin11
              slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                     5*fmin9 + 6*fmin10 + 7*fmin11

              int ~ warmth"

joint.lik.fit <- growth(joint.lik, 
                      data = adversity,
                      estimator = "ML",
                      missing = "FIML")

summary(joint.lik.fit, fit.measures = FALSE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

We can see that while the dataset has a total of $398$ individuals, only $361$ are being used in the actual model fit. While in the MLM, this would be the price of admission, in the SEM we can utilize the *joint* rather than the conditional likelihood which instead estimates a mean and variance for the exogenous variable (essentially treating it as an *endogenous* variable with no predictors). There are two ways to accomplish this. In general, we can simply explicitly include a variance and intercept term for the exogenous variable(s) in the model syntax (this works across programs). We can see this below.

```{r tic 04, message = FALSE, warning = FALSE, error = FALSE}
joint.lik <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                     1*fmin9 + 1*fmin10 + 1*fmin11
              slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                     5*fmin9 + 6*fmin10 + 7*fmin11

              int ~ warmth
               
              warmth ~~ warmth
              warmth ~ 1"

joint.lik.fit <- growth(joint.lik, 
                      data = adversity,
                      estimator = "ML",
                      missing = "FIML")

summary(joint.lik.fit, fit.measures = FALSE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

In **lavaan** we have the additional option to use the argument `fixed.x = FALSE` which will accomplish the same thing without the additional lines of syntax.

```{r tic 05, message = FALSE, warning = FALSE, error = FALSE}
joint.lik <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                     1*fmin9 + 1*fmin10 + 1*fmin11
              slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                     5*fmin9 + 6*fmin10 + 7*fmin11

              int ~ warmth"

joint.lik.fit <- growth(joint.lik, 
                      data = adversity,
                      estimator = "ML",
                      missing = "FIML",
                      fixed.x = FALSE)

summary(joint.lik.fit, fit.measures = FALSE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

In both instances, we can see that the model utilizes all observations rather than only those with non-missing values on `warmth`. One particular application where this may be of interest is when doing a larger behavioral study with a smaller neuroimaging component. If the brain measures are predictors, than the joint likelihood approach allows us to estimate the behavioral parameters from the larger dataset when including brain-based variables.

Another unique strength of SEMs is the ability to leverage the power of latent variables at all levels of the model. Observed TICs are by-far the most common application of these models, but there is nothing stopping us from attenuating measurement error in the TIC as well using a measurement model. We can see this below where we regress the intercept on a latent measure of cognitive stimulation in the home (measured by observed variables: `cog1` - `cog4`). Unlike the growth-related latent variables, here we allow the factor loadings on the latent `cog` variable to be freely-estimated.

```{r tic 06, message = FALSE, warning = FALSE, error = FALSE}
latent.tic <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                      1*fmin9 + 1*fmin10 + 1*fmin11
               slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                      5*fmin9 + 6*fmin10 + 7*fmin11
                          
               cog =~ cog1 + cog2 + cog3 + cog4
               cog ~~ 1*cog
               cog ~ 0*1

               int ~ cog"

latent.tic.fit <- growth(latent.tic, 
                         data = adversity,
                         estimator = "ML",
                         missing = "FIML")

summary(latent.tic.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

You might wonder why bother estimating a new latent variable rather than simply summing up the `cog` items and regressing the intercept on the observed measure. Well we can do this below and compare the results to the latent TIC approach.

```{r tic 07, message = FALSE, warning = FALSE, error = FALSE}
adversity = adversity %>%
  mutate(cog = cog1 + cog2 + cog3 + cog4)

obs.tic <- "int =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 1*fmin8 +
                   1*fmin9 + 1*fmin10 + 1*fmin11
            slp =~ 0*fmin4 + 1*fmin5 + 2*fmin6 + 3*fmin7 + 4*fmin8 +
                   5*fmin9 + 6*fmin10 + 7*fmin11

            int ~ cog"

obs.tic.fit <- growth(obs.tic, 
                      data = adversity,
                      estimator = "ML",
                      missing = "FIML")

summary(obs.tic.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

adversity <- adversity %>% select(-cog)
```

Focusing one the regression of `int ~ cog` in both models, we can compare the standardized coefficients and see that the observed measure reduces the effect quite substantially (latent TIC: $\beta = 0.466$, observed TIC: $\beta = 0.296$). Since we simulated this measure a, we knew that the true standardized effect should be $\beta_{population} = 0.500$, highlighting the utility of latent variables to attenuate the effects of measurement error that often bias effects downwards.

### Cross-Level Interactions

Up until now, we have limited our discussion of TICs to include only those that predicted the intercept. This is because when we combine level 1 TVC variables with level 2 TIC variables, we can begin generating cross level interactions. We have not yet discussed TVCs in-depth (we do so below), in growth models the most common cross-level interactions are between TICs and our time variable (here `age`). As such, we will introuce these effects here before going on to explore TVCs in a more general way. Cross-level interactions are easier to see with the MLM, as the interaction is often not immediately apparent in SEMs. Below we will introduce a treatment TIC (`tx`) to our earlier TIC model. Here we will switch our TIC of interest to be the variable `abuse` for...reasons (it is a better demonstration). We will first only predict the intercept as we did previously and then we will fit a cross-level interaction model.

```{r crosslevel 01, message = FALSE, warning = FALSE, error = FALSE}
tic.mlm <- lmer(fmin ~ 1 + age + abuse + (1 + age | id),
                     na.action = na.omit,
                     REML = TRUE,
                     data = adversity.long,
                     control = lmerControl(optimizer = "bobyqa",
                                           optCtrl = list(maxfun = 2e5)))

summary(tic.mlm, correlation = FALSE)
```

When we add `abuse` as a predictor, we are really only estimating intercept differences across levels of experience of abuse (there is an negative effect here). If we want to know whether individuals with different experiences of abuse differ in their slopes, we need to explicitly create an interaction variable (`age:abuse`).

```{r crosslevel 02, message = FALSE, warning = FALSE, error = FALSE}
cross.mlm <- lmer(fmin ~ 1 + age + abuse + age:abuse + (1 + age | id),
                  na.action = na.omit,
                  REML = TRUE,
                  data = adversity.long,
                  control = lmerControl(optimizer = "bobyqa",
                                        optCtrl = list(maxfun = 2e5)))

summary(cross.mlm, correlation = FALSE)
```

The interaction nature of the second effect is immediately apparent given the construction of the syntax. We can even plot these effects using the `interact_plot()` function from the **interactions** package.

```{r crosslevel 03, message = FALSE, warning = FALSE, error = FALSE}
interact_plot(model = cross.mlm, 
              pred = "age",  
              modx = "abuse", 
              interval = TRUE, 
              int.type = "confidence", 
              x.label = "Age",
              y.label = "Forcept Minor White Matter FA")
```

Here we can see some (non-significant) level differences between those who experience low levels of abuse ($-1$ SD) versus those who experienced mean or high ($+1$ SD) levels of abuse. However, there are significant slope differences (i.e., the interaction term is significant), such that those who experience lower levels of early childhood abuse also show faster gains in white matter FA. For some additional examples that have both intercept and slope differences, you can consult the help tools for the **interactions** package [here](https://interactions.jacob-long.com/reference/interact_plot.html).

In an SEM approach, seeing *how* to specify a cross-level interaction is easier, although the interaction nature is somewhat more obscured. Implementing the model is trivial, we simply predict both the intercept and slope with our TIC. We again retain the TVC effects for consistency, even though they are not strictly necessary to demonstrate a cross-level interaction.

```{r crosslevel 04, message = FALSE, warning = FALSE, error = FALSE}
cross.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
              slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

              ef1 ~ c*dlpfc1
              ef2 ~ c*dlpfc2
              ef3 ~ c*dlpfc3
              ef4 ~ dlpfc4
            
              int ~ tx
              slp ~ tx"

cross.lcm.fit <- growth(cross.lcm,
                        data = executive.function,
                        estimator = "ML",
                        missing = "FIML")

summary(cross.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

Unlike with the MLM, there is no product variable that we enter explicitly into the model. However, we can think about how the slope factor links the TIC and the individual repeated measures. The factor loadings are increasing integers and so the effect of the TIC on each successive repeated measure gets moderated by the numerical value of the factor loading. This serves the same function as an interaction for the SEM approaches.

### Time Varying Covariates

While TICs can provide important insights into what factors might contribute to individual differences in the growth trajectories, they fundamentally represent cross-sectional regressions of the latent factors on a set of exogenous variables. However, if we want to test the effects of variables that themselves change over time, we can instead move the TVC model. While TICs predict the growth components (and therefore the repeated measures *indirectly*; hence cross-level interactions), a TVC directly predicts the individual repeated observations. In this section, we will return to our `executive.function` dataset. Here we can model the impact of DLPFC activation on executive function scores. As with the TIC model, we will first consider the MLM specification of this approach.

```{r tvc 01, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long <- executive.function %>% 
  pivot_longer(cols = starts_with(c("dlpfc","ef","age")), 
               names_to = c(".value", "wave"), 
               names_pattern = "(.+)(.)") %>%
  mutate(wave = as.numeric(wave) - 1,
         age = age - min(age, na.rm = TRUE))

tvc.mlm <- lmer(ef ~ 1 + age + dlpfc + (1 + age | id),
                na.action = na.omit,
                REML = TRUE,
                data = executive.function.long,
                control = lmerControl(optimizer = "bobyqa",
                                      optCtrl = list(maxfun = 2e5)))

summary(tvc.mlm, correlation = FALSE)
```

We know have a variable that changes *within* (i.e., across time) as well as *between* individuals. Here we have actually somewhat blurred that line but we will talk about how to separate these within- and between-person effects towards the end of this chapter. However, for now we can see that there is a positive effect where more DLPFC activation predicts higher executive function scores. Here this is a purely fixed effect meaning that it holds across all individuals. As with the effect of `age`; however, we can introduce a random effect of the TVC as well but including it in the random effects structure. This addition allows for the possibility that increasing DLPFC activation has *differential* effects within different individuals.

```{r tvc 02, message = FALSE, warning = FALSE, error = FALSE}
rtvc.mlm <- lmer(ef ~ 1 + age + dlpfc + (1 + age + dlpfc | id),
                 na.action = na.omit,
                 REML = TRUE,
                 data = executive.function.long,
                 control = lmerControl(optimizer = "bobyqa",
                                       optCtrl = list(maxfun = 2e5)))

summary(rtvc.mlm, correlation = FALSE)
```

Here we've added $3$ additional elements in our random effects covariance matrix, one additional variance, and two additional covariances with the effects we had in the prior model. We can see that the correlation between the random slope (`age`) and DLPFC activation (`dlpfc`) is quite high. This can lead to estimation issues if this correlation gets too high, and generally reflects redundancy among the random effects.

While the model above already gives us the effect of DLPFC on executive funciton scores above and beyond the growth trajectory, the effects are all contemporaneous. That is, predicting executive function at a given time with DLPFC activation from that same time. A more interesting model is one in which we used activation from one time point to predict executive function at a *later* time point. While relatively rare, these lagged-effect models really represent a more exciting use of the trouble we went to in order to collect these variables over time. Because of the long format used in MLM, including lagged effects necessitates the creation of a new DLFPC variable with values offset one time point. Fortunately **dplyr** provides a convenient function to accomplish this data management step. It is; however, important to remember to group by `id` before creating this new variable so the last value of `dlpfc` for one person does not shift to the next person's data rows. In this model, we will drop the random effect of `dlpfc` for simplicity (and it's redundancy in the prior model).

```{r tvc 03, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long <- executive.function.long %>% 
  group_by(id) %>% 
  mutate(dlpfc.lag = dplyr::lag(dlpfc, 1))

lag.tvc.mlm <- lmer(ef ~ 1 + age + dlpfc + dlpfc.lag + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(lag.tvc.mlm, correlation = FALSE)
```

Here the effect of the lagged DLPFC variable is non-significant (arbitrary thresholds are arbitrary but live by $p < .05$, die by $p < .05$; just be consistent). However, one concerning thing about the lagged MLM model is that we have a reduced sample size compared to the contemporaneous model (contemporaneous: N obs = $1241$ & N `id` = $342$; lagged: N obs = $892$ & N `id` = $330$). This loss of information is not trivial and comes from two sources. It is helpful to visualize the data frame, which we have done below.

```{r tvc 04, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long %>% filter(id <= 15 & id >= 10) %>%
  kable(label = NA,
        format = "html",
        digits = 3,
        booktabs = TRUE,
        escape = FALSE,
        caption = "**Executive Function Data with Lagged Variable**",
        align = "c",
        row.names = FALSE) %>%
  row_spec(row = 0, align = "c")
```

Note that for every person in our data, we have `NA` values for the first observation of the `dlpfc.lag` variable. This is because we could not observe DLPFC activation the time point *before* we began the study. Because of the conditional likelihood approach we discussed previous as the only option in the MLM, all of these first observation rows are removed from the model. If that wasn't enough of a bummer, any missing data in the contemporaneous DLPFC variable are shifted to be missing in the lagged version for the *next* time point, so we lose both time points when we include the contemporaneous and lagged effects (which we should pretty much always do to properly specify the model, so don't be tempted to only include the lagged effect). We can see this effect for `id` $13$ where DLPFC is missing at time $3$ and so the lagged variable is missing at time $4$. One potential solution proposed by [Dan McNeish]() is that we replace the `NA` values for the first observation with a constant, $0$, so that we don't lose those time points but they also do not introduce spurious effects. However, we do not want to remove `NA` values that arise due to true missing data.

```{r tvc 05, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long[executive.function.long$wave == 0, "dlpfc.lag"] <- 0

lag.tvc.mlm <- lmer(ef ~ 1 + age + dlpfc + dlpfc.lag + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(lag.tvc.mlm, correlation = FALSE)
```

As we can see, the number of observations and `id` values is much more similar to the contemporaneous effects model. The lag effect is still not significant, but them's the breaks. Lag effects are often difficult to detect in these kind of longitudinal models due to the relatively long intervals between observations. Relationships simply decay over time and therefore observations spaced out in year or biennial schedules often don't result in unique lagged effects.

We can also briefly return to the idea of cross-level interactions here. While in the growth model context, we are primarily concerned with cross-level interactions with whatever metric of time we are utilizing in the model, we are not limited to this kind of effect and can moderate the effect of any level 1 TVC with a level 2 TIC. Below might be an example with `tx` and `dlpfc`.

```{r crosslevel 05, message = FALSE, warning = FALSE, error = FALSE}
cross.mlm <- lmer(ef ~ 1 + age + dlpfc + tx + dlpfc:tx + (1 + age | id),
                  na.action = na.omit,
                  REML = TRUE,
                  data = executive.function.long,
                  control = lmerControl(optimizer = "bobyqa",
                                        optCtrl = list(maxfun = 2e5)))

summary(cross.mlm, correlation = FALSE)

interact_plot(model = cross.mlm, 
              pred = "dlpfc",  
              modx = "tx", 
              interval = TRUE, 
              int.type = "confidence", 
              x.label = "DLPFC Activation",
              y.label = "Executive Function", 
              modx.labels = c("Treatment", "Control"))
```

The effects here are not significant, but it is a good principle to keep in mind, especially if fitting these models in a non-growth context.

If we turn to the SEMs, we can re-create many of the models we saw with the MLM (although not exclusively in R), but the SEM does allow for some additional flexibility that may be attractive. Because each measure gets its own column at different time points (i.e., wide format), we need to specify each of the contemporaneous (or lagged) effects individually. However, we will not need to explicitly create a lagged version of the variable. We can see the contemporaneous effect model below. Note that each time-specific regression of `ef` on `dlpfc` requires its own line of syntax.

```{r tvc 06, message = FALSE, warning = FALSE, error = FALSE}
tvc.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
            slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

            ef1 ~ dlpfc1
            ef2 ~ dlpfc2
            ef3 ~ dlpfc3
            ef4 ~ dlpfc4"

tvc.lcm.fit <- growth(tvc.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")

summary(tvc.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

The striking thing here, compared with the MLM results, is that we get $4$ regression coefficients for the relationship between `ef` and `dlpfc` instead of $1$. These individual time-specific effects are one of the unique strengths of the SEM approaches, and would be essentially impossible to achieve with most MLMs (**Mplus** is a bit of a potential exception, but that's because it specifies MLMs more like SEMs), since this is a weird form of nonlinear interaction between the overall TVC effect and time. However, we could also impose simpler functional forms on these regressions. The simplest is to constrain all of them to be equal. This constraint is the one imposed in the MLM, so we should get a single overall TVC effect with this approach. We can see this below.

```{r tvc 07, message = FALSE, warning = FALSE, error = FALSE}
tvc.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
            slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

            ef1 ~ c*dlpfc1
            ef2 ~ c*dlpfc2
            ef3 ~ c*dlpfc3
            ef4 ~ c*dlpfc4"

tvc.lcm.fit <- growth(tvc.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")

summary(tvc.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

If we examine the TVC regressions, they are all precisely equal (it should worry us if they are not) and equivalent to the results from the MLM. It is possible through the use of nonlinear constraints to put other functional forms on these regressions. For instance, we could impose a linear constraint so that the regressions increase or decrease over time. We could also impose order constraints where we want the regressions at some time points to be larger than at others. This is where the incredible flexibility of SEM approaches comes out most strongly. The important thing to keep in mind is that this flexibility is also a great way to wander and over-fit our way off a cliff. We need good theoretical justification for our model specification choices and a greater willingness to admit when some features of our model are exploratory to avoid these pitfalls.

One current limitation of implementing SEM TVC models in R is in the inability to specify a random effect of the TVC (to our knowledge). This is possible in **Mplus** and we have included syntax for that model in the external software downloads.

Finally, the lagged effect model is simple to implement in SEM approaches. We simply include the earlier time point version of our TVC in the regressions for the time-specific outcomes. We won't need to worry about inducing additional missing data because we never created a lagged version of our TVC.

```{r tvc 08, message = FALSE, warning = FALSE, error = FALSE}
lag.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
            slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

            ef1 ~ dlpfc1
            ef2 ~ dlpfc2 + dlpfc1
            ef3 ~ dlpfc3 + dlpfc2
            ef4 ~ dlpfc4 + dlpfc3"

lag.lcm.fit <- growth(lag.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")

summary(lag.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

Finally, we could constrain the different types of pathways to be equal within type. While the flexibility of the time-specific effects can be powerful, it can also overwhelm us inferentially. What do we do with the fact that only one of our paths here (`ef4 ~ dlpfc3`) is significant? Who knows. Constraints can both narrow the number of parameters we are interpreting, but also increase the power to detect an overall effect if one exists because we aggregate over more information.

```{r tvc 09, message = FALSE, warning = FALSE, error = FALSE}
lag.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
            slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

            ef1 ~ c1*dlpfc1
            ef2 ~ c1*dlpfc2 + c2*dlpfc1
            ef3 ~ c1*dlpfc3 + c2*dlpfc2
            ef4 ~ c1*dlpfc4 + c2*dlpfc3"

lag.lcm.fit <- growth(lag.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")

summary(lag.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

This aggregation of course cuts both ways. Now we have no significant lagged effect (although we only had one out of three before so did we really have it to begin with?), but we do have a significant contemporaneous effect where we had no individual one before.

The examples we have included thus far are relatively simple, with a single TVC. Of course all of these models can be expanded to include multiple TVCs, and potentially interactions if we have the sample size to support them.

### Multivariate Models

Those paying close attention (and at this point all the kudos to you) might have been a bit perturbed by the fact that in our [Canonical](#canon) chapter, we fit a growth model to the `dlpfc` variable, but here we have used it as our TVC. This is indeed of great concern and something we should always be thinking about when including a repeated measure as a TVC. The standard TVC model assumes that there are no systematic changes in the covariate across time. The values obviously change, but not systematically. Rather, the traditional TVC model is well suited for covariates that fluctuates from time point to time point. Weather over short intervals is often a canonical example in more-intensive applications, but we can think also the time of day that testing occurs as one that might be of interest for our purposes. From our earlier model fitting, we know that our `dlpfc` TVC does not meet this criteria. This is an issue because we essentially have an unmodeled age effect on the TVC, which will introduce bias into our model results.

The other issue is that of true exogeneity. Do we really believe that the causal effect in our model *only* runs from DLPFC activation to executive function, and that there are no reciprocal relationships over time? With something like the weather this is totally plausible; however, with our data this assumption seems unlikely to hold. As such, we might wish to treat both `ef` and `dlpfc` as outcomes in a multivariate model. This will involve some difficulty in the MLM, but will be a completely natural extension of the SEM. We will flip our usual order of considering the MLM first and demonstrate the SEM specifications so it's easier to see where we will run into difficulty with the MLMs. We will first re-fit the TVC model we ended with earlier

```{r multivar 01, message = FALSE, warning = FALSE, error = FALSE}
tvc.lcm <- "int =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
            slp =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4

            ef1 ~ c*dlpfc1
            ef2 ~ c*dlpfc2
            ef3 ~ c*dlpfc3
            ef4 ~ c*dlpfc4"

tvc.lcm.fit <- growth(tvc.lcm, 
                      data = executive.function,
                      estimator = "ML",
                      missing = "FIML")

summary(tvc.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

To transform this model into a multivariate version, we need to estimate an intercept and slope for both the `ef` and the `dlpfc` variables. One thing to note is that we should be explicit about including the correlations between the growth factors within- and across-construct, as well as cross-construct, time-specific covariances between the individual repeated measures. These time-specific residuals are often necessary and help account for unmeasured common causes that might induce correlations among the repeated measures above-and-beyond the growth factors themselves. We can do all the usual things with these residual covariances, including testing equality constraints or even constraining them to zero and assess their impact on model fit through likelihood ratio tests. For now we will estimate unique relationships, as that is the default in SEM approaches.

```{r multivar 02, message = FALSE, warning = FALSE, error = FALSE}
multivar.lcm <- "
                 # Growth Factors for EF
                 int.ef =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
                 slp.ef =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4
                 
                 # Growth Factors for DLPFC
                 int.dlpfc =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
                 slp.dlpfc =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4
                 
                 # Factor Covariances
                 int.ef ~~ slp.ef + int.dlpfc + slp.dlpfc
                 slp.ef ~~ int.dlpfc + slp.dlpfc
                 int.dlpfc ~~ slp.dlpfc
                 
                 # Time-Specific Residual Covariance
                 ef1 ~~ dlpfc1
                 ef2 ~~ dlpfc2
                 ef3 ~~ dlpfc3
                 ef4 ~~ dlpfc4"

multivar.lcm.fit <- growth(multivar.lcm, 
                           data = executive.function,
                           estimator = "ML",
                           missing = "FIML")

summary(multivar.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

This model is quite simple for a SEM approach, and we can examine how different constructs travel together over time by examining the inter-relations among the growth factors. However, this is ultimately cross-sectional correlations because there is no temporal order at the latent level, either between intercepts and slope or between factors of different constructs. If we want to examine cross-lagged relationships among the repeated measures themselves (ala the standard cross-lag panel model we won't consider here for numerous reasons), we might wish to introduce regression relationships among the repeated measures.

There are several ways to specify these regression among the repeated measures in a multivariate growth model; however, we will focus on what is known as the random-intercept cross-lag panel model (RI-CLPM) and it's generalization in the latent curve model with structured residuals (LCM-SR). For an overview of potential alternatives, see [Usami & Hamaker, 2019](https://psycnet.apa.org/fulltext/2019-21491-001.html). We retain the time-specific residual covariances, but now we include regression coefficients between the residuals of the repeated measures (hence "structured residuals"). Note that for the RI-CLPM, we remove the slope factors. In the LCM-SR, we can include any combination of latent growth factors, so that will be the term we will use to reference this general type of model, with the RI-CLPM being a special case.

```{r multivar 03, message = FALSE, warning = FALSE, error = FALSE}
riclpm <- "
          # Random Intercept for EF
          int.ef =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
                 
          # Random Intercept for DLPFC
          int.dlpfc =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
                 
          # Factor Covariances
          int.ef ~~ int.dlpfc
                 
          # Time-Specific Residual Covariance
          ef1 ~~ dlpfc1
          ef2 ~~ dlpfc2
          ef3 ~~ dlpfc3
          ef4 ~~ dlpfc4
              
          # Autoregressive and Cross-Regressive Effects
          ef2 ~ ef1 + dlpfc1
          ef3 ~ ef2 + dlpfc2
          ef4 ~ ef3 + dlpfc3
          dlpfc2 ~ dlpfc1 + ef1
          dlpfc3 ~ dlpfc2 + ef2
          dlpfc4 ~ dlpfc3 + ef3"

riclpm.fit <- growth(riclpm,
                     data = executive.function,
                     estimator = "ML",
                     missing = "FIML")

summary(riclpm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

With the LCM-SR, we simply add back the slope factors for each of our constructs. Note that in practice, we do not need to specify the same type of trajectory for each variable. It's entirely possible that one variable might follow a linear and the other a quadratic. Indeed one construct might only need a random intercept (we will see this later); we can mix and match as we please.

```{r multivar 04, message = FALSE, warning = FALSE, error = FALSE}
lcmsr <- "
         # Growth Factors for EF
         int.ef =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
         slp.ef =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4
                 
         # Growth Factors for DLPFC
         int.dlpfc =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
         slp.dlpfc =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4
                 
         # Factor Covariances
         int.ef ~~ slp.ef + int.dlpfc + slp.dlpfc
         slp.ef ~~ int.dlpfc + slp.dlpfc
         int.dlpfc ~~ slp.dlpfc
                 
         # Time-Specific Residual Covariance
         ef1 ~~ dlpfc1
         ef2 ~~ dlpfc2
         ef3 ~~ dlpfc3
         ef4 ~~ dlpfc4
              
         # Autoregressive and Cross-Regressive Effects
         ef2 ~ ef1 + dlpfc1
         ef3 ~ ef2 + dlpfc2
         ef4 ~ ef3 + dlpfc3
         dlpfc2 ~ dlpfc1 + ef1
         dlpfc3 ~ dlpfc2 + ef2
         dlpfc4 ~ dlpfc3 + ef3"

lcmsr.fit <- growth(lcmsr,
                    data = executive.function,
                    estimator = "ML",
                    missing = "FIML")

summary(lcmsr.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

For all the reasons we have discussed so far (and common practice in CLPMs more generally), it is common to test equality constraints on the auto- and cross-regressive effects in these models. We can see an example of this below.

```{r multivar 05, message = FALSE, warning = FALSE, error = FALSE}
lcmsr <- "
         # Growth Factors for EF
         int.ef =~ 1*ef1 + 1*ef2 + 1*ef3 + 1*ef4
         slp.ef =~ 0*ef1 + 1*ef2 + 2*ef3 + 3*ef4
                 
         # Growth Factors for DLPFC
         int.dlpfc =~ 1*dlpfc1 + 1*dlpfc2 + 1*dlpfc3 + 1*dlpfc4
         slp.dlpfc =~ 0*dlpfc1 + 1*dlpfc2 + 2*dlpfc3 + 3*dlpfc4
                 
         # Factor Covariances
         int.ef ~~ slp.ef + int.dlpfc + slp.dlpfc
         slp.ef ~~ int.dlpfc + slp.dlpfc
         int.dlpfc ~~ slp.dlpfc
                 
         # Time-Specific Residual Covariance
         ef1 ~~ dlpfc1
         ef2 ~~ dlpfc2
         ef3 ~~ dlpfc3
         ef4 ~~ dlpfc4
              
         # Autoregressive and Cross-Regressive Effects
         ef2 ~ c1*ef1 + c3*dlpfc1
         ef3 ~ c1*ef2 + c3*dlpfc2
         ef4 ~ c1*ef3 + c3*dlpfc3
         dlpfc2 ~ c2*dlpfc1 + c4*ef1
         dlpfc3 ~ c2*dlpfc2 + c4*ef2
         dlpfc4 ~ c2*dlpfc3 + c4*ef3"

lcmsr.fit <- growth(lcmsr,
                    data = executive.function,
                    estimator = "ML",
                    missing = "FIML")

summary(lcmsr.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

As always, we can extend this model in a number of ways that we won't go into here for sake of time (ha ha ha you may say to yourself). While uncommon, we could impose functions on the cross-regressive paths, add additional outcome variables, and include both TIC and (true) TVCs to this model. The limitations are as always, proper theoretical models for that complexity, and sufficient data to estimate all of these effects uniquely.

Multivariate LCSMs have some additional peculiarities that we will explore since they diverge somewhat from the LCM we have mostly focused on. However, as with all SEMs, the problem is much more keeping track of the parameter syntax than it is an inability to estimate certain effects. The main point of divergence between the LCM and LCSM is the inclusion of proportional parameters in the multivariate model. Because of this, we will dive straight into that model instead of building up in the way we did with the LCM. Below we can see the syntax for this multivariate dual-change LCSM.

```{r multivar 06, message = FALSE, warning = FALSE, error = FALSE}
multivar.dualchange <- "
        # Define Phantom Variables (p = phantom)
        pef1 =~ 1*ef1; ef1 ~ 0; ef1 ~~ ef1; pef1 ~~ 0*pef1
        pef2 =~ 1*ef2; ef2 ~ 0; ef2 ~~ ef2; pef2 ~~ 0*pef2
        pef3 =~ 1*ef3; ef3 ~ 0; ef3 ~~ ef3; pef3 ~~ 0*pef3
        pef4 =~ 1*ef4; ef4 ~ 0; ef4 ~~ ef4; pef4 ~~ 0*pef4
        
        pdlpfc1 =~ 1*dlpfc1; dlpfc1 ~ 0; dlpfc1 ~~ dlpfc1; pdlpfc1 ~~ 0*pdlpfc1
        pdlpfc2 =~ 1*dlpfc2; dlpfc2 ~ 0; dlpfc2 ~~ dlpfc2; pdlpfc2 ~~ 0*pdlpfc2
        pdlpfc3 =~ 1*dlpfc3; dlpfc3 ~ 0; dlpfc3 ~~ dlpfc3; pdlpfc3 ~~ 0*pdlpfc3
        pdlpfc4 =~ 1*dlpfc4; dlpfc4 ~ 0; dlpfc4 ~~ dlpfc4; pdlpfc4 ~~ 0*pdlpfc4
        
        # Regressions Between Adjacent Observations
        pef2 ~ 1*pef1
        pef3 ~ 1*pef2
        pef4 ~ 1*pef3
        
        pdlpfc2 ~ 1*pdlpfc1
        pdlpfc3 ~ 1*pdlpfc2
        pdlpfc4 ~ 1*pdlpfc3
        
        # Define Change Latent Variables (d)
        def21 =~ 1*pef2; def21 ~~ 0*def21
        def32 =~ 1*pef3; def32 ~~ 0*def32
        def43 =~ 1*pef4; def43 ~~ 0*def43
        
        ddlpfc21 =~ 1*pdlpfc2; ddlpfc21 ~~ 0*ddlpfc21
        ddlpfc32 =~ 1*pdlpfc3; ddlpfc32 ~~ 0*ddlpfc32
        ddlpfc43 =~ 1*pdlpfc4; ddlpfc43 ~~ 0*ddlpfc43
               
        # Define Proportional Change Regressions (beta = equality constraint)
        def21 ~ beta1*pef1 + beta3*pdlpfc1
        def32 ~ beta1*pef2 + beta3*pdlpfc2
        def43 ~ beta1*pef3 + beta3*pdlpfc3
        
        ddlpfc21 ~ beta2*pdlpfc1 + beta4*pef1
        ddlpfc32 ~ beta2*pdlpfc2 + beta4*pef2
        ddlpfc43 ~ beta2*pdlpfc3 + beta4*pef3
        
        # Define Intercept and Slope
        int.ef =~ 1*pef1
        slp.ef =~ 1*def21 + 1*def32 + 1*def43
        int.ef ~ 1; int.ef ~~ int.ef
        slp.ef ~ 1; slp.ef ~~ slp.ef
        int.ef ~~ slp.ef
        
        int.dlpfc =~ 1*pdlpfc1
        slp.dlpfc =~ 1*ddlpfc21 + 1*ddlpfc32 + 1*ddlpfc43
        int.dlpfc ~ 1; int.dlpfc ~~ int.dlpfc
        slp.dlpfc ~ 1; slp.dlpfc ~~ slp.dlpfc
        int.dlpfc ~~ slp.dlpfc
        
        # Cross-Construct Covariances
        int.ef ~~ int.dlpfc + slp.dlpfc
        slp.ef ~~ int.dlpfc + slp.dlpfc
"

multivar.dualchange <- sem(multivar.dualchange, 
                           data = executive.function, 
                           estimator = "ML",
                           missing = "FIML")

summary(multivar.dualchange, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

This absolute wall of syntax specifies a really compelling model of change. Not only does change depend on the prior level within a variable, it also depends on the prior level of the other variable in the model. These non-linearities can provide a powerful and flexible tool to assess reciprocal relationships among variables as they travel together over time. Now that we have a taste of the power, we can turn to the process of fitting multivariate models in the MLM.

Unfortunately, the MLM is fundamentally a univariate model that we will need to trick in order to turn into a multivariate model. To do so, we need to do some data management to combine our two outcomes (`ef` and `dlpfc`) into a single column. However, to keep track of which values correspond to which outcome, we will also create a set of dummy codes that indicate the original variable. The easiest way to do this is to begin by stacking our data frame on top of itself and then generating these new variables.

```{r multivar 07, message = FALSE, warning = FALSE, error = FALSE}
multivar.long <- rbind(executive.function.long, executive.function.long)

multivar.long$dv <- c(executive.function.long$ef, executive.function.long$dlpfc)

multivar.long$ef_dummy <- c(rep(1, nrow(executive.function.long)),
                            rep(0, nrow(executive.function.long)))

multivar.long$dlpfc_dummy <- c(rep(0, nrow(executive.function.long)),
                               rep(1, nrow(executive.function.long)))

multivar.long$dv_factor <- factor(multivar.long$ef_dummy, 
                                  levels = c(0, 1),
                                  labels = c("DLPFC", "EF"))

multivar.long <- multivar.long %>%
  arrange(id)
```

It's helpful to visualize what this results in for our data. We can display the first three participants' data below.

```{r multivar 08, message = FALSE, warning = FALSE, error = FALSE}
multivar.long %>% filter(id <= 3) %>%
  kable(label = NA,
        format = "html",
        digits = 3,
        booktabs = TRUE,
        escape = FALSE,
        caption = "**Executive Function Multivariate Data**",
        align = "c",
        row.names = FALSE) %>%
  row_spec(row = 0, align = "c")
```

We've doubled the number of rows per participant and when `ef_dummy` $= 1$, the value of `dv` is the same as `ef` and the same as `dlpfc` when `ef_dummy` $= 0$. Creating the factor variable (`dv_factor`) is both to help us keep track of which value is which more easily and will play a role in structuring the covariance matrix in the model syntax.

In order to best approximate the full bivariate growth model we fit with the LCM, we need to fit a more-complex variance structure than is currently available in **lmer**. As such, we will return to **nlme** for this section. The syntax is a bit odd, so we will cover it in-depth before we fit the model. We will model the combined `dv` variable as a function of our constructed dummy variables and their interactions with `age`. We will not model an overall intercept (`dv ~ 0`) since `dv` is a combination of our two outcomes. Instead, our dummy codes (`ef_dummy` and `dlpfc_dummy`) will estimate our outcome-specific intercepts, and the interactions (`ef_dummy:age` and `dlpfc_dummy:age`) will estimate the outcome-specific slopes. We will include all of these effects in the `random` effects argument as well. We will estimate different residual variances for each outcome using the `weights` argument and nesting our estimates within our `dv_factor` (this must be a different variable than one that appears in our formula for some reason). We will updates some options in our `lmeControl()` function to aid in convergence.

```{r multivar 09, message = FALSE, warning = FALSE, error = FALSE}
lmeCtlList = lmeControl(maxIter = 10000, msMaxIter = 10000, tolerance = 1e-7, niterEM = 10000,
                        msMaxEval = 10000, msVerbose = FALSE, returnObject = FALSE,
                        gradHess = TRUE, opt = 'optim', optimMethod = 'Nelder-Mead')

bivar.mod = lme(dv ~ 0 + ef_dummy + ef_dummy:age + dlpfc_dummy + dlpfc_dummy:age,
                random = list(id = ~ 0 + ef_dummy + ef_dummy:age + 
                                dlpfc_dummy + dlpfc_dummy:age),
                weights = varIdent(form = ~ 1 | dv_factor),
                na.action = na.omit,
                method = 'REML',
                data = multivar.long,
                control = lmeCtlList)

summary(bivar.mod, correlation = FALSE)
```

While we do get the effects of interest with this model, there is an important remaining mis-specification. With the model above, we have to assume that the time-specific residuals are independent (covariances set to zero). This is a very untenable assumption, so we need to add a new argument to our model. The `correlation` argument in **nlme** allows us to correlate observations within a given `age`. Note that we nest that correlation within individual (`id`) and then age.

```{r multivar 10, message = FALSE, warning = FALSE, error = FALSE}
bivar.mod = lme(dv ~ 0 + ef_dummy + ef_dummy:age + dlpfc_dummy + dlpfc_dummy:age,
                random = list(id = ~ 0 + ef_dummy + ef_dummy:age + 
                                dlpfc_dummy + dlpfc_dummy:age),
                weights = varIdent(form = ~ 1 | dv_factor),
                correlation = corSymm(form= ~ 1 | id / age),
                na.action = na.omit,
                method = 'REML',
                data = multivar.long,
                control = lmeCtlList)

summary(bivar.mod, correlation = FALSE)
```

With this specification we do allow the residuals to correlate, but they are constrained to be equal across all of the time points in our model. Freeing this residual constraint to allow for unique correlations is possible, but requires us to fit the model in **SAS**. This model syntax will be provided in the downloadable files.

### Within- and Betwen-Person Variance

In longitudinal models, we are deeply interested in understanding within-person processes (i.e., how individuals change over time). However, we need to be on guard against *between*-person differences masquerading as these within-person effects. For instance, there is a substantively different understanding of adolescent substance use if it is guided by within-person effects (e.g., adolescent are more likely to use illicit drugs when they are with their peers) versus between-person effects (e.g., adolescents with more friends are more likely to use illicit drugs). Unfortunately, we can often unintentionally conflate these two types of effects in our models. This happens because variables at level $1$ (e.g., TVCs) contain information about both within- (level $1$) and between- (level $2$) person differences. To take our example thus far, individuals might show higher or lower DLPFC activation relative to the last time they played the task (i.e., within-person differences) but also some individuals might consistently show higher (or lower) than average DLFPC activation relative to other individuals in the sample (i.e., between-person differences). We can return to our contemporaneous TVC model to demonstrate this principle. Below is the model we fit before.

```{r withbetw 01, message = FALSE, warning = FALSE, error = FALSE}
uncens.mlm <- lmer(ef ~ 1 + age + dlpfc + (1 + age | id),
                   na.action = na.omit,
                   REML = TRUE,
                   data = executive.function.long,
                   control = lmerControl(optimizer = "bobyqa",
                                         optCtrl = list(maxfun = 2e5)))

summary(uncens.mlm, correlation = FALSE)
```

So now we know that the coefficient associated with the `dlpfc` predictor is a weighted combination of the within- and between-person effect. We can apply well-known principles of variable centering in the MLM in order to separate out these effects. For a more in-depth treatment of these approaches, see [Curran & Bauer (2011)](). The first approach is called **grand-mean** centering. Here we do something relatively intuitive and simply subtract the mean of all of the observations of `dlpfc` across individuals to create `dlpfc.grdcent`. This simply makes the zero-point of `dlpfc` represent the mean of the variable. Unsurprisingly, this simple linear transformation of our model will not change the model results except to change the interpretation of the intercept (i.e., the expected value of the outcome when all predictors are zero will change because the zero-point of `dlpfc.grdcent` has changed). However, the effect of `dlpfc.grdcent` will still confound the within- and between-person effects.

```{r withbetw 02, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long <- executive.function.long %>%
  mutate(dlpfc.grdcent = dlpfc - 
           mean(executive.function.long$dlpfc, na.rm = TRUE))

grdcent.mlm <- lmer(ef ~ 1 + age + dlpfc.grdcent + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(grdcent.mlm, correlation = FALSE)
```

In order to disaggregate these effects, we need to include the person-specific means of `dlpfc` as an additional predictor. Creating this variable is straightforward, we will just need to remember to do the mean operation *within-person* by using the `group_by(id)` function. The syntax for this model is below.

```{r withbetw 03, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long <- executive.function.long %>%
  group_by(id) %>%
  mutate(dlpfc.mean = mean(dlpfc, na.rm = TRUE))

grdcent.mlm <- lmer(ef ~ 1 + age + dlpfc.grdcent + dlpfc.mean + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(grdcent.mlm, correlation = FALSE)
```

By conditioning the effect of `dlpfc.grdcent` on the effect of `dlpfc.mean`, we get a pure estimate of the within-person effect of DLPFC activation on executive function scores. The effect of `dlpfc.mean` itself is a little harder to interpret. With grand-mean centering, the effect of `dlpfc.mean` represents the *difference* between the within- and between-person effect. If we wish to compute the between-person effect itself, we can linearly combine the parameter estimates using the `contest1D()` function from the **lmerTest** package. This function will compute both the point estimate and inferential test for the computed parameter. The vector of weights (`c(0, 0, 1, 1)`) tells the function to ignore the `(Intercept)` and `age` coefficients and to add the `dlpfc.grdcent` (within-person effect) and `dlpfc.mean` (between- minus within-person effect) coefficients together. Since the within-person effects cancel each other out in this equation, we are left with the pure between-person effect.

```{r withbetw 04, message = FALSE, warning = FALSE, error = FALSE}
lmerTest::contest1D(grdcent.mlm, c(0, 0, 1, 1))
```

We can see that the between-person effect ($\gamma = 0.123$) is substantially larger than the within-person effect ($\gamma = 0.040$).

While the grand-mean centering approach is relatively intuitive in the centering process (simply subtract a single value from all observations), the effects it produces are less intuitive. We must include the person-specific means in the model to get de-confounded effects, and the effect of the person-specific means is not always what we wish to interpret and we run the risk of mis-interpreting it as the between-person effect itself rather than the difference in effects. Thus, we might wish to consider an alternative centering approach.

This alternative is group-mean centering. Unlike grand-mean centering, here we will subtract a *different* value (namely the person-mean) from each person's observations. As such, our new predictor (`dlpfc.grpcent`) is fundamentally different from the original `dlpfc` variable because we have removed all mean differences between individuals from the new variable. Because we have discarded this information, the group-mean centered variable can give us a pure estimate of the within-person effect even if we do not include the group means in the model. As such, if we only care about the within-person effect, we can obtain that effect without needing to include the additional predictor in the model. We can see this model below.

```{r withbetw 05, message = FALSE, warning = FALSE, error = FALSE}
executive.function.long <- executive.function.long %>%
  mutate(dlpfc.grpcent = dlpfc - dlpfc.mean)

grpcent.mlm <- lmer(ef ~ 1 + age + dlpfc.grpcent + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(grpcent.mlm, correlation = FALSE)
```

If we are interested in both the within- and between-person effects; however, we can include the person-specific means (`dlpfc.mean`) as we did before. As an added bonus, the effect of `dlpfc.mean` is a pure estimate of the between-person effect without needing to do any additional transformations.

```{r withbetw 06, message = FALSE, warning = FALSE, error = FALSE}
grpcent.mlm <- lmer(ef ~ 1 + age + dlpfc.grpcent + dlpfc.mean + (1 + age | id),
                    na.action = na.omit,
                    REML = TRUE,
                    data = executive.function.long,
                    control = lmerControl(optimizer = "bobyqa",
                                          optCtrl = list(maxfun = 2e5)))

summary(grpcent.mlm, correlation = FALSE)
```

Of course, if we wished to get the difference in effects, we can compute it from this model (essentially the inverse of what we did previously). Here we subtract the within-person effect from the between-person effect.

```{r withbetw 07, message = FALSE, warning = FALSE, error = FALSE}
contest1D(grpcent.mlm, c(0, 0, -1, 1))
```

Fortunately for us, we have already seen one of the primary methods for separating within- versus between-person effects in the SEM: the LCM-SR. In these models, all of the between-person effects are isolated to the level of the latent factors, while the within-person effects are isolated to the residuals of the repeated measures. If we have a TVC model, our grand- or group-mean centered variables enter as preditors of the repeated measure residuals, while the person-specific means enter as predictors of the latent factors. While this is equivalent to the MLM in the case of TVCs, the expanded capacity of the SEM allows us to include multiple outcomes and reciprocal effects between different constructs traveling together across time. With the growth factors isolating the between-person effects, these reciprocal effects represent pure estimates of within-person effects.

## Distal Outcomes

Up until this point, we have considered how to include predictors in our growth models to explain individual differences in patterns of change over time. However, if we think that these differences in developmental trajectories *mean anything* (and we'd like to think they do), we might wish to predict some future outcome that is meaningful for our theoretical question of interest. For instance, do greater increases in executive function scores during adolescence predict greater education attainment, lifetime earnings, or reduced contact with criminal-legal institutions? Such questions are uncomfortably rare in developmental cognitive neuroscience, leaving a big opening for questions of "**so what?**" for our programs of research. Commensurate with their relative scarcity in substantive research, much less work has gone into the development of quantitative methods and best-practices for distal outcome prediction. As such, readers should take the methods presented in this section as a work-in-progress, awaiting further developments (some of which we can confidently say are in the works).

For this section, we will draw the `externalizing` data from our `external.math` data set. However, here we have included data from a follow-up study on lifetime incarceration as of age $35$. As such, we can predict the likelihood of an individual experiencing incarceration as an adult based on trajectory information from when they were in early adolescence.

```{r read data outcomes, message = FALSE, warning = FALSE, error = FALSE}
externalizing <- read.csv("data/externalizing-outcome.csv", header = TRUE)

externalizing.long <- externalizing %>% 
  pivot_longer(cols = starts_with("ext"), 
                      names_to = c(".value", "age"), 
                      names_pattern = "(ext)(.+)") %>%
  mutate(age = as.numeric(age))
```

In the MEMs, outcome prediction usually proceeds via a two-step procedure (with **Mplus** again being an exception for reasons we have already discussed). As such, we fit the growth model to the repeated measures data, obtain individual estimates of the random effects, and then predict the distal outcome in a second regression analysis. The linear model we fit will be familiar to use, and then we can use the `ranef()` function from **lme4** to obtain our estimates of the random effects. Note that these are Empirical Bayes (EB) estimates of the random effects, which have som enice statistical properties. For our purposes, we should know that EB estimates are "shrunken" meaning that individuals with more extreme values and those individuals with that contribute fewer data points to the model are shrunk towards the mean effect. This helps to reduce outliers and differences in reliability for individual estimates based on smaller numbers of repeated measures. We can see the code to obtain these effects below.

```{r outcomes 01, message = FALSE, warning = FALSE, error = FALSE}
distal.mlm <- lmer(ext ~ 1 + age + (1 + age | id),
                   na.action = na.omit,
                   REML = TRUE,
                   data = externalizing.long,
                   control = lmerControl(optimizer = "bobyqa",
                                         optCtrl = list(maxfun = 2e5)))

random.effs <- ranef(distal.mlm)$id
names(random.effs) <- c("int", "slp")

externalizing <- cbind(externalizing, random.effs)
```

Now that we have obtained these EB estimates, we can bring them into the secondary regression analysis. Here our outcome of lifetime incarceration (`incarc`) is binary (\$0 = \$ "never incarcerated", \$1 = \$ "ever incarcerated"), so we will utilize the `glm()` function ("generalized linear model") instead of `lm()` ("general linear model") to deal with the discrete nature of our outcome. While we won't spend much time on the distinctions between linear and nonlinear models, see [here](https://quantdev.ssri.psu.edu/tutorials/regression-continuous-count-and-binary-outcomes) for a great general review of the topic in R.

```{r outcomes 02, message = FALSE, warning = FALSE, error = FALSE}
distal.glm <- glm(incarc ~ 1 + int + slp,
                  family = "binomial",
                  data = externalizing)

summary(distal.glm)
externalizing <- externalizing %>% select(-c(int, slp))
```

We can see that while the intercept of the trajectory has a significant positive prospective effect on the propensity for individuals to experience lifetime incarceration, the slope is not significant (so close; more on that in a bit). Essentially, those adolescents who show higher initial levels of antisocial behavior are more likely to report being incarcerated at some point in their lifetime. The coefficients we see in this model represent the log-odds parameters. You might see these reported in the odds scale, which just involves the exponentiation of these parameters. We can see this below and generate confidence intervals on the odds parameters.

```{r outcomes 03, message = FALSE, warning = FALSE, error = FALSE}
exp(cbind(coef(distal.glm), confint(distal.glm)))
```

So for a one unit increase in the trajectory intercept, we expect a 17.72 change in the odds of being incarcerated, and a substantial increase of 301.85 in the odds for a per-unit change in the slope of the trajectory (again not significant but we can play out this exercise). One reason we often need to be careful about these "per one unit" changes in the trajectory parameters, especially for the slope, is that often the parameters do not have large ranges. Indeed, often the full range of the slope parameter will not extend over an entire unit because of the way it is scaled (in per time-unit change). We can see this when we plot the predicted probabilities of incarceration against the trajectory parameters. Note that to get the predicted probabilities, we simply divide the odds by $1$ plus the odds. So $P(incarc_{i} = 1) = e^{log-odds} / 1 + e^{log-odds}$. We can see this below.

```{r outcomes 04, message = FALSE, warning = FALSE, error = FALSE}
random.effs <- random.effs %>%
  mutate(incarc = externalizing$incarc,
         pred = stats::predict.glm(distal.glm)) %>%
  mutate(pred.prob = exp(pred) / (1 + exp(pred)))
head(random.effs)

ggplot(random.effs, aes(x = int, y = incarc)) +
  geom_point(aes(color = factor(incarc)), shape = "|", size = 3) +  
  geom_point(data = random.effs, 
             mapping = aes(x = int, y = pred.prob, color = factor(incarc))) +  
  ylim(0, 1) + 
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE) +
  labs(title = "Plotting Predicted Probabilities of Lifetime Incarceration",
       x = "Intercept (Wave 1) of Adolescent Antisocial Behavior",
       y = "Predicted Probability of Incarceration") + 
  theme(legend.position = "none")

ggplot(random.effs, aes(x = slp, y = incarc)) +
  geom_point(aes(color = factor(incarc)), shape = "|", size = 3) +  
  geom_point(data = random.effs, 
             mapping = aes(x = slp, y = pred.prob, color = factor(incarc))) +  
  ylim(0, 1) + 
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE) +
  labs(title = "Plotting Predicted Probabilities of Lifetime Incarceration",
       x = "Slope of Adolescent Antisocial Behavior",
       y = "Predicted Probability of Incarceration") + 
  theme(legend.position = "none")
```

As we can see, the slope parameter in particular does not cover a full 1 unit range, and most of the observations are between $-0.2$ and $0.2$. Here we have plotted the predicted probabilities of incarceration (dots) against each of the parameters, color-coded by their observed response on the incarceration item (red = "No", green = "Yes"). Note that there is some mismatch, especially in the middle-zones of the plots, where some of those who have *actually* experienced incarceration have low model-implied probabilities, and vice versa. This is perfectly normal and means we simply haven't included all of the multitude of relevant predictors for lifetime incarceration (which is completely unsurprising given what we know about incarceration patterns in the population). We will not go much further on diagnostics here, but just to note: there is a single point at the very high end of the graph that seems to be anchoring the curve fairly strongly. It would be a great idea to test the model sensitivity to excluding this person from the analysis.

Of course, it was a bit of a bummer that the slope term wasn't quite significant (and yes of course we shouldn't care, but **of course** we do). Slope effects are often very interesting from an intervention perspective, with the idea that if we could just bend those curves down during adolescence, perhaps we could avoid some of these negative life outcomes. Well tough cookies in this model, but one of the reasons that we do not get a slope effect for this data in particular is because of the high degree of correlation between the EB estimates of the random effects, which is quite high ($r =$ `r round(cor(random.effs[,1:2])[2,1], 3)`). Similar to the ideas outlined in [multi-growth models](https://mccormickneuro.github.io/publication/2021-multilevel-multigrowth/), the correlation in predictors here isn't doing us any favors. We can check the variance inflation factor for this model below using the `vif()` function from the [**car**](https://cran.r-project.org/web/packages/car/index.html) package.

```{r outcomes 05, message = FALSE, warning = FALSE, error = FALSE}
car::vif(distal.glm)
```

Which indicates that our standard errors are approximately $2.77$ larger than if we had uncorrelated predictors. While this isn't enormous, it likely contributes to a non-significant slope effect. This type of correlation between the random effects is not uncommon and is one of the challenges associated with these kinds of questions (although there are some developments that I am currently working on that might help with this, so stay tuned if interested).

We can do a very similar 2-step approach with the LCM by generating factor scores. Here we will use the `lavPredict()` function from **lavaan** with `method = "regression"`. The other option is to use `method = "Bartlett"`. In general, regression factor scores perform better as predictors (which is what we want here), while Bartlett scores are better as outcomes ([Skrondal & Laak, 2001](https://psycnet.apa.org/record/2001-10245-007)).

```{r outcomes 06, message = FALSE, warning = FALSE, error = FALSE}
lin.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
"

lin.lcm.fit <- growth(lin.lcm, 
                      data = externalizing,
                      estimator = "ML",
                      missing = "FIML")

factor.scores <- lavPredict(lin.lcm.fit, 
                            type = "lv",
                            method = "regression")

externalizing <- cbind(externalizing, factor.scores)
```

We can then perform the same regression for step $2$.

```{r outcomes 07, message = FALSE, warning = FALSE, error = FALSE}
distal.glm <- glm(incarc ~ 1 + int + slp,
                  family = "binomial",
                  data = externalizing)

summary(distal.glm)
externalizing <- externalizing %>% select(-c(int, slp))
```
Now our slope effect is no-where near significance. The correlation between the factor scores here is even higher ($r =$ `r round(cor(factor.scores)[2,1], 3)`) so our VIF is `r round(car::vif(distal.glm)[1], 3)` which might contribute to that. However, part of it is that factor scores and EM estimates are different ways of estimating an unobserved quantity so we will get different results depending on our approach. One common alternative in SEMs is to go ahead and include the distal outcome into the same model that estimates the trajectory and estimate everything simultaneously we can see the syntax for this model below. The inclusion of the new parameters are trivial from a syntax perspective; we just add the regression directly. However, because `incarc` is a binary variable, we need to include new options in the `growth()` function. Specifically, we change the estimator to `"WLSMV"` (see [here](https://lavaan.ugent.be/tutorial/est.html) for details on different estimator options) and indicate that `incarc` is a categorical variable with `ordered = "incarc"`.

```{r outcomes 08, message = FALSE, warning = FALSE, error = FALSE, eval = FALSE}
sim.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
            
            incarc ~ 1 + int + slp
"

sim.lcm.fit <- growth(sim.lcm, 
                      data = externalizing,
                      estimator = "WLSMV",
                      ordered = "incarc")
```

Unfortunately, when we try to estimate this model, we get an error message `lavaan ERROR: data contains no observations`, which should be slighly concerning... What is happening here is that in this dataset, because no one person is observed on adjacent measures (e.g., `ext6` & `ext7`), everyone has missing data. This was not an issue in the linear versions of the model because we could use Maximum Likelihood, however, currently **lavaan** does not support categorical ML, and WLSMV will listwise delete individuals with missing data. Unfortunately for us, that's $100$ percent of adolescents in our sample. We would need to turn to **Mplus** or another program for results from this model.

As we mentioned, this area of methodological research is still relatively under-developed and awaits solutions to many of the limitations we have run into here.

```{r, cleanup covariates, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```

<!--chapter:end:05-covariates.Rmd-->

# Nested Data {#nesting}

The final topic we will consider is the way we account for nesting within longitudinal data. While technically all of longitudinal modeling involves nested data (i.e., multiple observations nested within the same individual), here we focus more on between-person nesting groups (e.g., classroom, family, data collection sites). As we will see, there are several forms of the models that we have already used that deal with nesting. The `achieve` dataset we will work with here are from a 4-wave school-based assessment of math and science achievement across ages $12$ - $17$. Schools were drawn from $5$ different metropolitan areas and assessments were conducted by separate research teams in each city. Here we will mostly focus on the science achievement data, but may draw examples from math achievement if they differ in interesting ways.

```{r read data nesting, message = FALSE, warning = FALSE, error = FALSE}
achieve <- read.csv("data/achieve.csv", header = TRUE)

achieve.wide <- achieve %>%
  pivot_wider(id_cols = c("site", "school", "id", "male"), 
              names_from = "wave",
              values_from = c("sci", "math"),
              names_sep = ".",
              values_fill = NA)
```

As always, we can fit an unconditional growth model to give us a baseline of expectations for the effects we will in subsequent models. Here we will fit the MLM and LCM versions as they are the simplest representations.

## Unconditional Model
```{r unc01, message = FALSE, warning = FALSE, error = FALSE}
unc.mlm <- lmer(sci ~ 1 + wave + (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)

summary(unc.mlm, correlation = FALSE)
```

```{r ucn02, message = FALSE, warning = FALSE, error = FALSE}
unc.lcm <- "int =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
            slp =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4"

unc.lcm.fit <- growth(unc.lcm,
                      data = achieve.wide,
                      estimator = "ML",
                      missing = "FIML")

summary(unc.lcm.fit, fit.measures = TRUE, estimates = TRUE, 
        standardize = TRUE, rsquare = TRUE)
```

As a general takeaway, we have an average science achievement of around $60$ (these scores are arbitrarily scaled so that isn't incredibly informative) but more of interest, science achievement is increasing over time approximately $2$ points per year of school.

## Categorical Predictors
Nesting induces correlations between observations because members of the same group are more similar to one another than members of other groups. The practical upshot of these models is that we obtain different values of parameters across group (although the methods for this will vary). Perhaps the simplest method for doing so, and therefore accounting for group differences, is to predict the mean of the outcome variable using a categorical predictor. In longitudinal modeling, this most often occurs at the level of the individual as a TIC. As such, we already know how to fit this kind of model from the last chapter. We can show this syntax again below. Here we predict the intercept and slope random effects with `male` to examine differences between self-reported males and females. In the MLM syntax, we simply include `male` in the regression equation if we want to predict mean differences in science achievement.

```{r cat01, message = FALSE, warning = FALSE, error = FALSE}
cat.mlm.1 <- lmer(sci ~ 1 + wave + male + (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)

summary(cat.mlm.1, correlation = FALSE)
```

Here we see that males show $1.6$ units higher science achievement compared with females (the reference category). It is possible to predict other parameters in the model, although the options in mixed-effects models tend to be more limited compared with SEM approaches. For instance, if we thought error variance might differ across groups, we could include this in our MLM syntax through the variance identity (`varIdent`) function. The `form` equation, `~ 1 | male`, mimics the regression equation where we have a separate "intercept" (here just read this as estimate) nested within group (`male`). We can then go through the procedure we covered in Chapter 3 to recover the different estimates for males and females (here in standard deviations, but we can easily square these values to obtain variances).

```{r cat02, message = FALSE, warning = FALSE, error = FALSE}
cat.mlm.2 <- lme(sci ~ 1 + wave + male,
                 random = ~ 1 + wave | id,
                 weights = varIdent(form = ~ 1 | male),
                 na.action = na.omit,
                 method = "REML",
                 data = achieve)

summary(cat.mlm.2)

c(cat.mlm.2$sigma, cat.mlm.2$sigma * exp(as.numeric(cat.mlm.2$modelStruct$varStruct)))
```

These results show that males appear to have increased variability in their science achievement compared to females in addition to a higher average. Note that we are not getting a direct effect estimate here, but rather estimating values separately for each group. There are not clear ways of implementing that kind of effect in **lme4**, so we will not go too much further except to mention that within the mixed-effect world of models, you can directly model predictors of variability with location-scale models, although those generally require more intensive longitudinal kinds of data. As such, we will not consider this further.

Because these categorical variables enter the model at the person level, in the mixed-effects models they predict the random effects rather than the individual time-specific observations. However, in the SEM, we can additionally predict one (or more) of the individual repeated measures. We can see this approach in a version of a MIMIC model, where we predict both the latent factors and the first repeated measure (`sci.1`).

```{r cat03, message = FALSE, warning = FALSE, error = FALSE}
cat.lcm <- "int =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
            slp =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4

            int ~ male
            slp ~ male
            
            sci.1 ~ male"

cat.lcm.fit <- growth(cat.lcm,
                      data = achieve.wide,
                      estimator = "ML",
                      missing = "FIML")

summary(cat.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

These approaches are relatively simple ways to incorporate nesting information into the model, but highlight the continuity with more complex approaches we will discuss as we move forward in this chapter.

## Multiple Groups Models

While the categorical predictor allows us to vary some parameters across groups, a unique power of the SEM is the multiple group model, where we can estimate unique parameters of essentially any type across any number of groups (although the number of groups tends to be small in practice). The reason for a small number of groups tends to be due to sample size requirements within-group. SEM is still a large-sample estimator in the multiple groups model, so we have to be careful we aren't under-powering the model at the group level, even if the overall sample size is large. Below we can see how the multiple-group LCM is specified. Nothing changes about the syntax object `mult.g1` from what we have seen before, however, now we include the argument `group = "male"` to indicate that we want to obtain unique estimates for females and males. Typically, these models require setting some small set of parameters equal in the model just to identify the model, so here we conveniently achieve this through having the same set factor loadings for the growth model. We can see the results below.

```{r multg01, message = FALSE, warning = FALSE, error = FALSE}
mult.g1 <- "int =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
            slp =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4"

mult.g1.fit <- growth(mult.g1,
                      data = achieve.wide,
                      group = "male",
                      estimator = "ML",
                      missing = "FIML")

summary(mult.g1.fit, fit.measures = TRUE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

We can see that we now have separate test statistics and sample sizes that reflect the fact that we are - to a large extent - estimating separate models. We also can see that our parameter estimates are broken up into `Group 1 [1]` and `Group 2 [0]`. The value in brackets represents the value of the grouping variable `male`, so here Group 1 is males and Group 2 is female (you could change this variable into a labeled factor and then those labels would be more informative). It's important to note the ordering of the groups for future steps.

Here we can qualitatively say that among other things, males seem to start higher ($61.6$ vs. $60.1$) and increase more rapidly ($2.22$ vs. $1.90$) in their achievement, and have a higher correlation between starting point and rate of change ($r = 0.226$ vs. $r = .105$), compared with females. However, a natural question is whether these differences are significant. We can explicitly test differences across groups by introducing group-specific labels into our model syntax and building composite parameters where we get a formal inferential test on the difference. A really nice feature of maximum likelihood parameters is that linear operations on the parameter are themselves maximum likelihood parameters themselves. To achieve this, we simply pre-multiply a given parameter by a vector of labels (e.g., `c(Mi, Fi)`) and then define the composite parameter using the `:=` operator. Here we will test the difference between the intercept (`c(Mi, Fi)`) and slope factor means (`c(Ms, Fs)`), and the covariance between factors (`c(Mc, Fc)`). This is where keeping track of the group order becomes important so we don't mis-label our parameters of interest and reverse our inference. The `D*` composite parameters are then created by simply subtracting `F*` from the `M*` parameters.

```{r multg02, message = FALSE, warning = FALSE, error = FALSE}
mult.g2 <- "int =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
            slp =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4
            
            int ~ c(Mi, Fi)*1
            slp ~ c(Ms, Fs)*1
            
            int ~~ c(Mc, Fc)*slp
            
            Di := Mi - Fi
            Ds := Ms - Fs
            Dc := Mc - Fc"

mult.g2.fit <- growth(mult.g2,
                      data = achieve.wide,
                      group = "male",
                      estimator = "ML",
                      missing = "FIML")

summary(mult.g2.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

We will only focus here on the `Defined Parameters` section, where we can see that while males have higher absolute estimates on all of these parameters we have tested, only the mean of the intercept factor significantly differs across groups. These are only a subset we chose for demonstration purposes, and we could explore others if we chose.

However, this model allows maximally different estimates, which belies one of the real strengths of the multiple-groups model, which is to selectively set a subset of parameters equal across groups. This allows us to bridge the gap between fitting the model in each group uniquely and fitting a single-group model where we ignore nesting altogether. The parameters we set equal benefit from increased power because it is not diluted by the smaller sub-samples and reduce the number of parameters we need to interpret. One common approach is to start with the type of model we first fit, with all parameters unequal, and then progressively build in equality constraints and test whether they reduce the fit of the model. We can do this in two ways, either through the syntax (changing labels like `c(Mi, Fi)` to be the same like `c(i, i)`) or through the argument `group.equal` in the model fitting function. The first approach gives us more control, but can be tedious in large models, while the latter is less precise but quick. For instance, if we want to constrain the factor means equal across groups, we can simply include `means` in the `group.equal` argument. We can see the results below.

```{r multg03, message = FALSE, warning = FALSE, error = FALSE}
mult.g3 <- "int =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
           slp =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4"

mult.g3.fit <- growth(mult.g3,
                      data = achieve.wide,
                      group = "male",
                      group.equal = c("means"),
                      estimator = "ML",
                      missing = "FIML")

summary(mult.g3.fit, fit.measures = TRUE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

Now note that the factor means (labels `.20.` and `.21.`) are equal across males and females. We could mix and match syntax and argument constraints if we like; like many things in SEM, we can have full control as long as we keep things organized. We can test whether this more constrained model fits significantly worse than the maximally-different model using a likelihood ratio test.

```{r multg04, message = FALSE, warning = FALSE, error = FALSE}
lavTestLRT(mult.g1.fit, mult.g3.fit)
```
Here it appears that constraining the factor means does significantly decrease model fit, so we wouldn't conclude that this constraint is appropriate. We could do this kind of testing with other parameters if we chose, although we won't here for space and simplicity. The complexity and utility of the multiple groups models is not something we can explore fully here, and interested readers can find a much larger literature on these kinds of models if they wish to implement them in their own research (see the manuscript for a general starting point).

## Fixed Effect Approach
The fixed effect approach is (rarely for quantitative methods) exactly what it sounds like in the context of mixed-effect models, we simply include our grouping variable as a fixed effect in the model. This has the convenience of account for unmeasured or otherwise unmodeled differences between groups that might bias the effects. In the most common use of the fixed-effects approach, we account for intercept differences by including a main effect of the group. In the context of our data example, we will model the effect of `site` on science achievement using a fixed approach. Within the `lmer()` syntax, we can use the short-cute of `factor(site)` to generate a series of dummy-codes, where `site == 1` is the reference category. We can see the results of this below.

```{r fix01, message = FALSE, warning = FALSE, error = FALSE}
fixed.mlm1 <- lmer(sci ~ 1 + wave + factor(site) + (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)
summary(fixed.mlm1, correlation = FALSE)
```

Note that the `(Intercept)` term is the intercept for Site 1 and each of the effects `factor(site)*` indicate the relative difference from that term for the other sites. If we wished to use an absolute coding scheme, we change the `1` to a `0` (indicating that we don't wish to estimate a reference intercept) and re-estimate the model.

```{r fix02, message = FALSE, warning = FALSE, error = FALSE}
fixed.mlm2 <- lmer(sci ~ 0 + wave + factor(site) + (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)
summary(fixed.mlm2, correlation = FALSE)
```

These results are equivalent to the first coding scheme but we get the intercept estimate for each site  instead of computing differences from a chosen site (note the change in what the inference test tells us).

Like mentioned above, the fixed effect approach is most commonly used for intercept differences, but can logically be extended to account for effect differences across sites (or another grouping variable). We can accomplish this by modeling the interaction of site with our time predictor like below.

```{r fix03, message = FALSE, warning = FALSE, error = FALSE}
fixed.mlm3 <- lmer(sci ~ 1 + wave + factor(site) + wave:factor(site) + (1 + wave | id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)
summary(fixed.mlm3, correlation = FALSE)
```

Here we return to the reference coding to test whether the effect of wave differs across sites from the reference. We can see how parameter do tend to multiply rapidly, however, with a relatively small set of grouping levels (here we only have 5 site), this is manageable. With larger numbers of groups (e.g., families), this approach would become less tenable. Note that we still have our random effect across individuals, but we've removed site-level differences from those estimates through the use of the fixed effects approach. Very often we will not interpret the associated parameters, but include them to de-confound our estimates of interest. Another thing to mention is that this isn't a "pure" form of the fixed-effects approach, since we still include the random effect of individual. The purely fixed-effect approach is more common/appropriate for group-based clustered (e.g., site, country) rather than individual-based clustering (as we always have in longitudinal data) since we cannot include individual level fixed-effects without breaking the model (it isn't identified).

## Random Effect Approach
If we want to move into accounting for grouping variables with many levels, the random effects framework provides a nice and ready solution. Indeed, we have been using it for the nesting of repeated measures within individuals this whole time. Within MEMs, we can include higher levels of nesting by expanding the structure of the random effects in our model syntax. Here we expand our classic `(1 + wave | id)` to include the higher level of nesting individuals within schools `(1 + wave | school/id)`.

```{r rnd01, message = FALSE, warning = FALSE, error = FALSE}
rand.mlm <- lmer(sci ~ 1 + wave + (1 + wave | school/id),
                na.action = na.omit,
                REML = TRUE,
                data = achieve)

summary(rand.mlm, correlation = FALSE)
```

The ordering of `school/id` is important otherwise we have schools nested within individual which will make the model sad (it's true). We can check that we've done this in the model summary where we have $1200$ individuals nested within schools (`id:school`) and $41$ schools. If we compare this model to the model where we ignore nesting within school, we will notice that the fixed effects have not changed that much, but that our estimate of individual-level variance has been broken up into individual- and school-level variability. This situation is fairly common (although not guaranteed), with fixed effects that are relatively robust to omitting higher levels of nesting, but mis-attributed variance estimates at lower levels of nesting that over-estimate inter-individual variance.

Compared with the fairly natural extension of the MEMs to higher levels of nesting, multilevel SEM estimation is much more challenging and the current options in R are fairly limited. For instance, **lavaan** currently only allows complete case continuous data with a random intercept at the higher level of nesting (see the current [tutorial](https://lavaan.ugent.be/tutorial/multilevel.html)). This is an active area of improvement so additional options are likely to be available soon. We provide **Mplus** examples of the full model that more closely resembles the MLM above. However, for completeness, we fit the model we can in **lavaan** below.

```{r rnd02, message = FALSE, warning = FALSE, error = FALSE}
mlsem <- "
           level: 1
           int.w =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
           slp.w =~ 0*sci.1 + 1*sci.2 + 2*sci.3 + 3*sci.4
           
           sci.1 ~~ a*sci.1
           sci.2 ~~ a*sci.2
           sci.3 ~~ a*sci.3
           sci.4 ~~ a*sci.4
           
           level: 2
           int.b =~ 1*sci.1 + 1*sci.2 + 1*sci.3 + 1*sci.4
           
           sci.1 ~~ b*sci.1
           sci.2 ~~ b*sci.2
           sci.3 ~~ b*sci.3
           sci.4 ~~ b*sci.4"



mlsem.fit <- growth(mlsem,
                    data = achieve.wide,
                    cluster = "school",
                    estimator = "ML",
                    missing = "listwise",
                    optim.method = "em")

summary(mlsem.fit, fit.measures = TRUE, estimates = TRUE, 
        standardize = TRUE, rsquare = TRUE)
```

Here we get a similar layout for the results as the multiple groups model, with the within estimates first, and then the school-level estimates next. We needed to set the residuals to be homoscedastic to achieve convergence here, but in principle these can be allowed to vary.

## Cluster Correction
The fixed effects point estimates within the model tend to be robust to the omission of higher levels of nesting, however, their standard errors do tend to be inaccurate. As such, sometimes we might wish to simply not interpret the variance components and focus on regression coefficients or other parts of our model. This might be less frequent in growth modeling, but for completeness we will consider this type of application. In this instance, we can simply implement a cluster-correction to our standard errors and then proceed as usual to interpret the model parameters. To our knowledge, this is not currently an option in R, so we provide the **Mplus** syntax to do so below.

```{r, cleanup nesting, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```

<!--chapter:end:06-nesting.Rmd-->

# Datasets {#datasets}

Use the button below to download all datasets used in this primer. Note that these datasets have been either **simulated or synthesized** and therefore **are not to be used to test substantive research hypotheses**.

```{r download, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
download_file(
  path = c("data/executive-function.csv",
           "data/feedback-learning.csv",
           "data/single-cohort.csv",
           "data/multiple-cohort.csv",
           "data/multiple-cohort-long.csv",
           "data/accelerated.csv",
           "data/adversity.csv",
           "data/external-math.csv",
           "data/trials.csv",
           "data/externalizing-outcome.csv",
           "data/achieve.csv"),
  output_name = "codebook-datasets",
  button_label = "Download Codebook Datasets",
  button_type = "default",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

## Descriptions

### Executive Function

The `executive.function` dataset is a single-cohort longitudinal design, consisting of $342$ adolescents, ages $11.74 - 15.33$, assessed annually across $4$ waves. The repeated measures of interest are DLPFC activation during an executive function task (`dlpfc*`) and behavioral scores on that task (`ef*`). Additional variables include time-invariant covariates, self-identified sex (`sex`) and assigned treatment group (`tx`), as well as age at observation (`age*`). A subset of this dataset is used as the `single.cohort` data in Chapter 3.

### Feedback Learning

The `feedback.learning` dataset is an accelerated longitudinal design, consisting of $297$ young people, ages $8.01 - 28.72$, assessed annually across $3$ waves. The repeated measures of interest are brain network modularity (`modularity`) and behavioral performance (`learning.rate`) during a feedback learning task. Additional variables include the time-invariant covariate of self-identified sex (`male`) and time-varying covariates including, wave of assessment (`wave`), pubertal status (`puberty`), and IQ (`iq`). This dataset is used as the `accelerated` dataset in Chapter 3. These data were synthesized from data analyzed in [McCormick et al., 2021](https://mccormickneuro.github.io/publication/2021-longitudinal-reorganization/). the real data used in that manuscript is provided in the supplemental material. 

### Externalizing and Math

The `external.math` dataset is a multiple-cohort longitudinal design, consisting of $405$ adolescents, ages $6 - 14$, measured bi-annually across $4$ waves. The repeated measures of interest are measures of externalizing behavior (`ext*`) and math achievement (`math*`).

### Adversity

The `adversity` dataset is a multiple-cohort longitudinal design, consisting of $398$ children, ages $4 - 11$, measured bi-annually across $4$ waves. The repeated measure of interest is the white matter fractional anisotropy of the forceps minor tract (`fmin*`). Additional variables include a set of time-invariant covariates: self-identifed sex (`sex`), experiences of early-childhood abuse (`abuse`), experiences of early-childhood neglect (`neglect`), levels of early-childhood parental warmth (`warmth`), and four measures of cognitive stimulation (`cog*`). This dataset is used as the `multiple.cohort` dataset in Chapter 3.

### Trials

The `trials` dataset contains trial-level responses from the feedback learning task that was used in the `feedback.learning` dataset. The repeated measures include $7$ trials for $297$ individuals.

### Achieve

The `achieve` dataset is a single-cohort longitudinal design, consisting of $1237$ adolescents, ages $12 - 17$, sampled across $4$ waves from $41$ schools in $5$ geographic locations (`site`). The repeated measures of interest include math (`math`) and science (`sci`) achievement scores. The dataset includes an additional time-invariant covariate of self-identified sex (`male`).


<!--chapter:end:07-datasets.Rmd-->

